{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InfoExtract.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EX_yNkPz25hs",
        "lwMEgg7z4-0R",
        "XKZSrUID5e9b",
        "TpCmuHtH6heD",
        "jZ2jy_nCAyer",
        "hWuyVCWm2HmV",
        "dsgUG5082yfF",
        "bfLUsknjQnmX",
        "dylJGXl1_F8W"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRQ7SVizo7ob",
        "colab_type": "text"
      },
      "source": [
        "# 0 Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZqkBox9wOtj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40ec63f0-0496-4127-fd29-5f789b4bb5a4"
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "import zipfile\n",
        "import json\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "!unzip -j /content/reports.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/reports.zip\n",
            " extracting: icsma-20-184-01.txt     \n",
            " extracting: icsa-20-184-01.txt      \n",
            " extracting: icsa-20-184-02.txt      \n",
            " extracting: icsa-20-182-01.txt      \n",
            " extracting: icsa-20-182-02.txt      \n",
            " extracting: icsma-20-177-01.txt     \n",
            " extracting: icsa-20-177-01.txt      \n",
            " extracting: icsa-20-177-02.txt      \n",
            " extracting: icsa-20-177-03.txt      \n",
            " extracting: icsa-20-175-01.txt      \n",
            " extracting: icsa-20-175-02.txt      \n",
            " extracting: icsa-20-175-03.txt      \n",
            " extracting: icsma-20-170-01.txt     \n",
            " extracting: icsma-20-170-02.txt     \n",
            " extracting: icsma-20-170-03.txt     \n",
            " extracting: icsma-20-170-04.txt     \n",
            " extracting: icsma-20-170-06.txt     \n",
            " extracting: icsma-20-170-05.txt     \n",
            " extracting: icsa-20-170-02.txt      \n",
            " extracting: icsa-20-170-03.txt      \n",
            " extracting: icsa-20-170-04.txt      \n",
            " extracting: icsa-20-170-05.txt      \n",
            " extracting: icsa-20-168-01.txt      \n",
            " extracting: icsa-20-170-01.txt      \n",
            " extracting: icsa-20-161-02.txt      \n",
            " extracting: icsma-20-163-01.txt     \n",
            " extracting: icsa-20-163-01.txt      \n",
            " extracting: icsa-20-163-02.txt      \n",
            " extracting: icsa-20-161-01.txt      \n",
            " extracting: icsa-20-161-03.txt      \n",
            " extracting: icsa-20-161-04.txt      \n",
            " extracting: icsa-20-161-05.txt      \n",
            " extracting: icsa-20-161-06.txt      \n",
            " extracting: icsa-20-133-02.txt      \n",
            " extracting: icsa-19-253-03.txt      \n",
            " extracting: ICSA-19-099-06.txt      \n",
            " extracting: ICSMA-18-228-01.txt     \n",
            " extracting: ICSMA-19-080-01.txt     \n",
            " extracting: icsa-20-154-01.txt      \n",
            " extracting: icsa-20-154-02.txt      \n",
            " extracting: icsa-20-154-03.txt      \n",
            " extracting: icsa-20-154-04.txt      \n",
            " extracting: icsa-20-154-05.txt      \n",
            " extracting: icsa-20-154-06.txt      \n",
            " extracting: icsa-20-147-01.txt      \n",
            " extracting: icsa-20-147-02.txt      \n",
            " extracting: icsa-20-142-01.txt      \n",
            " extracting: icsa-20-142-02.txt      \n",
            " extracting: icsa-20-140-01.txt      \n",
            " extracting: icsa-20-140-02.txt      \n",
            " extracting: icsa-20-135-01.txt      \n",
            " extracting: icsa-20-135-02.txt      \n",
            " extracting: icsa-19-213-04.txt      \n",
            " extracting: icsa-20-133-01.txt      \n",
            " extracting: icsa-20-105-05.txt      \n",
            " extracting: icsa-20-105-08.txt      \n",
            " extracting: icsa-20-042-06.txt      \n",
            " extracting: icsa-19-274-01.txt      \n",
            " extracting: icsa-19-255-02.txt      \n",
            " extracting: icsa-19-227-04.txt      \n",
            " extracting: icsa-19-190-05.txt      \n",
            " extracting: icsa-20-128-01.txt      \n",
            " extracting: ICSA2012601.txt         \n",
            " extracting: ICSA2012602.txt         \n",
            " extracting: icsa-20-119-01.txt      \n",
            " extracting: ICSA-19-122-03.txt      \n",
            " extracting: icsa-20-112-01.txt      \n",
            " extracting: icsa-20-105-01.txt      \n",
            " extracting: icsa-20-105-02.txt      \n",
            " extracting: icsa-20-105-03.txt      \n",
            " extracting: icsa-20-105-04.txt      \n",
            " extracting: icsa-20-105-06.txt      \n",
            " extracting: icsa-20-105-07.txt      \n",
            " extracting: icsa-20-105-09.txt      \n",
            " extracting: icsa-20-042-05.txt      \n",
            " extracting: icsa-20-014-05.txt      \n",
            " extracting: icsa-19-283-02.txt      \n",
            " extracting: icsa-20-100-01.txt      \n",
            " extracting: icsa-20-098-01.txt      \n",
            " extracting: icsa-20-098-02.txt      \n",
            " extracting: icsa-20-098-03.txt      \n",
            " extracting: icsa-20-098-04.txt      \n",
            " extracting: icsa-20-098-05.txt      \n",
            " extracting: icsa-20-042-01.txt      \n",
            " extracting: icsa-20-093-01.txt      \n",
            " extracting: icsma-20-091-01.txt     \n",
            " extracting: icsa-20-091-01.txt      \n",
            " extracting: icsa-20-091-02.txt      \n",
            " extracting: icsa-20-016-01.txt      \n",
            " extracting: icsa-20-086-01.txt      \n",
            " extracting: icsa-20-084-01.txt      \n",
            " extracting: icsa-20-084-02.txt      \n",
            " extracting: icsma-20-079-01.txt     \n",
            " extracting: icsa-20-079-01.txt      \n",
            " extracting: icsa-20-077-01.txt      \n",
            " extracting: icsa-20-072-01.txt      \n",
            " extracting: icsa-20-072-02.txt      \n",
            " extracting: icsa-20-072-03.txt      \n",
            " extracting: icsa-20-070-01.txt      \n",
            " extracting: icsa-20-070-02.txt      \n",
            " extracting: icsa-20-070-03.txt      \n",
            " extracting: icsa-20-070-04.txt      \n",
            " extracting: icsa-20-070-05.txt      \n",
            " extracting: icsa-20-070-06.txt      \n",
            " extracting: icsa-20-042-04.txt      \n",
            " extracting: icsa-20-042-11.txt      \n",
            " extracting: icsa-19-351-02.txt      \n",
            " extracting: icsa-19-344-04.txt      \n",
            " extracting: icsa-19-344-06.txt      \n",
            " extracting: icsa-19-283-01.txt      \n",
            " extracting: ICSA-19-099-03.txt      \n",
            " extracting: ICSA-16-348-05.txt      \n",
            " extracting: icsa-20-065-01.txt      \n",
            " extracting: icsa-20-063-01.txt      \n",
            " extracting: icsa-20-063-02.txt      \n",
            " extracting: icsa-20-063-03.txt      \n",
            " extracting: icsa-20-063-04.txt      \n",
            " extracting: icsa-20-056-01.txt      \n",
            " extracting: icsa-20-056-02.txt      \n",
            " extracting: icsa-20-056-03.txt      \n",
            " extracting: icsa-20-056-04.txt      \n",
            " extracting: icsa-20-056-05.txt      \n",
            " extracting: icsa-20-051-01.txt      \n",
            " extracting: icsa-20-051-02.txt      \n",
            " extracting: icsa-20-051-03.txt      \n",
            " extracting: icsa-20-051-04.txt      \n",
            " extracting: icsma-20-049-01.txt     \n",
            " extracting: icsma-20-049-02.txt     \n",
            " extracting: icsa-20-049-01.txt      \n",
            " extracting: icsa-20-049-02.txt      \n",
            " extracting: icsa-20-044-01.txt      \n",
            " extracting: icsa-20-044-02.txt      \n",
            " extracting: icsa-20-042-02.txt      \n",
            " extracting: icsa-20-042-03.txt      \n",
            " extracting: icsa-20-042-07.txt      \n",
            " extracting: icsa-20-042-08.txt      \n",
            " extracting: icsa-20-042-09.txt      \n",
            " extracting: icsa-20-042-10.txt      \n",
            " extracting: icsa-20-042-12.txt      \n",
            " extracting: icsa-20-042-13.txt      \n",
            " extracting: icsa-20-035-01.txt      \n",
            " extracting: ICSMA-18-058-01.txt     \n",
            " extracting: icsma-20-023-01.txt     \n",
            " extracting: icsa-20-021-01.txt      \n",
            " extracting: icsa-20-014-01.txt      \n",
            " extracting: icsa-20-014-02.txt      \n",
            " extracting: icsa-20-014-03.txt      \n",
            " extracting: icsa-20-014-04.txt      \n",
            " extracting: icsa-20-014-06.txt      \n",
            " extracting: icsa-19-344-07.txt      \n",
            " extracting: icsa-19-281-03.txt      \n",
            " extracting: ICSA-19-162-04.txt      \n",
            " extracting: ICSA-19-085-01.txt      \n",
            " extracting: ICSA-18-165-01.txt      \n",
            " extracting: ICSA-18-163-02.txt      \n",
            " extracting: icsma-19-274-01.txt     \n",
            " extracting: icsma-19-353-01.txt     \n",
            " extracting: icsa-19-353-01.txt      \n",
            " extracting: icsa-19-353-02.txt      \n",
            " extracting: icsa-19-353-03.txt      \n",
            " extracting: icsa-19-353-04.txt      \n",
            " extracting: icsa-19-318-04.txt      \n",
            " extracting: icsa-19-290-01.txt      \n",
            " extracting: icsa-19-351-01.txt      \n",
            " extracting: icsa-19-346-01.txt      \n",
            " extracting: icsa-19-346-02.txt      \n",
            " extracting: icsa-19-346-03.txt      \n",
            " extracting: icsma-19-318-01.txt     \n",
            " extracting: ICSA-19-106-03.txt      \n",
            " extracting: icsa-19-344-01.txt      \n",
            " extracting: icsa-19-344-02.txt      \n",
            " extracting: icsa-19-344-03.txt      \n",
            " extracting: icsa-19-344-05.txt      \n",
            " extracting: icsa-19-318-02.txt      \n",
            " extracting: ICSA-16-327-02.txt      \n",
            " extracting: ICSA-13-149-01.txt      \n",
            " extracting: icsa-19-339-01.txt      \n",
            " extracting: icsa-19-339-02.txt      \n",
            " extracting: icsa-19-337-01.txt      \n",
            " extracting: icsa-19-337-02.txt      \n",
            " extracting: icsa-19-330-01.txt      \n",
            " extracting: icsa-19-330-02.txt      \n",
            " extracting: icsa-19-323-01.txt      \n",
            " extracting: icsa-19-318-01.txt      \n",
            " extracting: icsa-19-318-03.txt      \n",
            " extracting: icsa-19-318-05.txt      \n",
            " extracting: ICSMA-19-120-01.txt     \n",
            " extracting: icsma-19-311-01.txt     \n",
            " extracting: icsma-19-311-02.txt     \n",
            " extracting: icsa-19-311-01.txt      \n",
            " extracting: icsa-19-311-02.txt      \n",
            " extracting: icsa-19-309-01.txt      \n",
            " extracting: ICSA-19-134-01.txt      \n",
            " extracting: icsa-19-304-01.txt      \n",
            " extracting: icsa-19-304-02.txt      \n",
            " extracting: icsa-19-304-03.txt      \n",
            " extracting: icsa-19-304-04.txt      \n",
            " extracting: icsa-19-302-01.txt      \n",
            " extracting: ICSA-19-057-01.txt      \n",
            " extracting: icsma-19-297-01.txt     \n",
            " extracting: icsa-19-297-01.txt      \n",
            " extracting: icsa-19-297-02.txt      \n",
            " extracting: icsa-19-295-01.txt      \n",
            " extracting: icsa-19-290-02.txt      \n",
            " extracting: icsa-19-192-02.txt      \n",
            " extracting: ICSA-19-134-08.txt      \n",
            " extracting: ICSMA-18-123-01.txt     \n",
            " extracting: ICSA-16-313-02.txt      \n",
            " extracting: icsa-19-281-01.txt      \n",
            " extracting: icsa-19-281-02.txt      \n",
            " extracting: icsa-19-281-04.txt      \n",
            " extracting: icsa-19-274-02.txt      \n",
            " extracting: icsa-19-274-03.txt      \n",
            " extracting: icsa-19-262-01.txt      \n",
            " extracting: icsa-19-260-01.txt      \n",
            " extracting: icsa-19-260-02.txt      \n",
            " extracting: icsa-19-260-03.txt      \n",
            " extracting: icsma-19-255-01.txt     \n",
            " extracting: icsa-19-255-01.txt      \n",
            " extracting: icsa-19-255-03.txt      \n",
            " extracting: icsa-19-255-04.txt      \n",
            " extracting: icsa-19-255-05.txt      \n",
            " extracting: icsa-19-253-01.txt      \n",
            " extracting: icsa-19-253-02.txt      \n",
            " extracting: icsa-19-253-04.txt      \n",
            " extracting: icsa-19-253-05.txt      \n",
            " extracting: icsa-19-253-06.txt      \n",
            " extracting: icsma-19-248-01.txt     \n",
            " extracting: icsa-19-248-01.txt      \n",
            " extracting: icsa-19-246-01.txt      \n",
            " extracting: icsa-19-246-02.txt      \n",
            " extracting: icsma-19-241-01.txt     \n",
            " extracting: icsma-19-241-02.txt     \n",
            " extracting: icsa-19-239-01.txt      \n",
            " extracting: icsa-19-239-02.txt      \n",
            " extracting: icsa-19-232-01.txt      \n",
            " extracting: icsa-19-227-01.txt      \n",
            " extracting: icsa-19-227-02.txt      \n",
            " extracting: icsa-19-227-03.txt      \n",
            " extracting: icsa-19-225-01.txt      \n",
            " extracting: icsa-19-225-02.txt      \n",
            " extracting: icsa-19-225-03.txt      \n",
            " extracting: icsa-19-213-01.txt      \n",
            " extracting: icsa-19-213-02.txt      \n",
            " extracting: icsa-19-213-03.txt      \n",
            " extracting: icsa-19-213-06.txt      \n",
            " extracting: icsa-19-213-05.txt      \n",
            " extracting: icsa-19-211-01.txt      \n",
            " extracting: icsa-19-211-02.txt      \n",
            " extracting: icsa-19-204-01.txt      \n",
            " extracting: icsa-19-204-02.txt      \n",
            " extracting: icsa-19-199-01.txt      \n",
            " extracting: icsma-19-192-01.txt     \n",
            " extracting: icsa-19-192-01.txt      \n",
            " extracting: icsa-19-192-03.txt      \n",
            " extracting: icsa-19-192-04.txt      \n",
            " extracting: icsa-19-192-05.txt      \n",
            " extracting: icsa-19-192-06.txt      \n",
            " extracting: icsa-19-192-07.txt      \n",
            " extracting: icsma-19-190-01.txt     \n",
            " extracting: icsa-19-190-01.txt      \n",
            " extracting: icsa-19-190-02.txt      \n",
            " extracting: icsa-19-190-03.txt      \n",
            " extracting: icsa-19-190-04.txt      \n",
            " extracting: icsa-19-183-01.txt      \n",
            " extracting: icsa-19-183-02.txt      \n",
            " extracting: icsma-19-178-01.txt     \n",
            " extracting: icsa-19-178-01.txt      \n",
            " extracting: icsa-19-178-02.txt      \n",
            " extracting: icsa-19-178-03.txt      \n",
            " extracting: icsa-19-178-04.txt      \n",
            " extracting: icsa-19-178-05.txt      \n",
            " extracting: ICSA-19-171-01.txt      \n",
            " extracting: ICSMA-19-164-01.txt     \n",
            " extracting: ICSA-19-164-01.txt      \n",
            " extracting: ICSA-19-164-02.txt      \n",
            " extracting: ICSA-19-162-01.txt      \n",
            " extracting: ICSA-19-162-02.txt      \n",
            " extracting: ICSA-19-162-03.txt      \n",
            " extracting: ICSA-19-157-01.txt      \n",
            " extracting: ICSA-19-157-02.txt      \n",
            " extracting: ICSA-19-155-01.txt      \n",
            " extracting: ICSA-19-155-02.txt      \n",
            " extracting: ICSA-19-155-03.txt      \n",
            " extracting: ICSA-19-150-01.txt      \n",
            " extracting: ICSA-19-148-01.txt      \n",
            " extracting: ICSA-19-141-01.txt      \n",
            " extracting: ICSA-19-141-02.txt      \n",
            " extracting: ICSA-19-136-01.txt      \n",
            " extracting: ICSA-19-136-02.txt      \n",
            " extracting: ICSA-19-134-02-0.txt    \n",
            " extracting: ICSA-19-134-03.txt      \n",
            " extracting: ICSA-19-134-04.txt      \n",
            " extracting: ICSA-19-134-05.txt      \n",
            " extracting: ICSA-19-134-06.txt      \n",
            " extracting: ICSA-19-134-07.txt      \n",
            " extracting: ICSA-19-134-09.txt      \n",
            " extracting: ICSA-19-122-01.txt      \n",
            " extracting: ICSA-19-122-02.txt      \n",
            " extracting: ICSA-19-120-01.txt      \n",
            " extracting: ICSMA-19-113-01.txt     \n",
            " extracting: ICSA-19-113-01.txt      \n",
            " extracting: ICSA-19-106-01.txt      \n",
            " extracting: ICSA-19-106-02.txt      \n",
            " extracting: ICSA-19-099-01.txt      \n",
            " extracting: ICSA-19-099-02.txt      \n",
            " extracting: ICSA-19-099-04.txt      \n",
            " extracting: ICSA-19-099-05.txt      \n",
            " extracting: ICSA-19-094-01.txt      \n",
            " extracting: ICSA-19-094-02.txt      \n",
            " extracting: ICSA-19-094-03.txt      \n",
            " extracting: ICSA-19-094-04.txt      \n",
            " extracting: ICSA-19-092-01.txt      \n",
            " extracting: ICSA-19-087-01.txt      \n",
            " extracting: ICSA-19-085-02.txt      \n",
            " extracting: ICSA-19-085-03-0.txt    \n",
            " extracting: ICSA-19-078-01.txt      \n",
            " extracting: ICSA-19-078-02.txt      \n",
            " extracting: ICSA-19-073-01.txt      \n",
            " extracting: ICSA-19-073-02.txt      \n",
            " extracting: ICSA-19-073-03.txt      \n",
            " extracting: ICSA-19-064-01.txt      \n",
            " extracting: ICSA-19-059-01.txt      \n",
            " extracting: ICSA-19-050-01.txt      \n",
            " extracting: ICSA-19-050-02.txt      \n",
            " extracting: ICSA-19-050-03.txt      \n",
            " extracting: ICSA-19-050-04.txt      \n",
            " extracting: ICSA-19-045-01.txt      \n",
            " extracting: ICSA-18-310-01.txt      \n",
            " extracting: ICSA-19-043-01.txt      \n",
            " extracting: ICSA-19-043-02.txt      \n",
            " extracting: ICSA-19-043-03.txt      \n",
            " extracting: ICSA-19-043-04.txt      \n",
            " extracting: ICSA-19-043-05.txt      \n",
            " extracting: ICSA-19-043-06.txt      \n",
            " extracting: ICSA-19-038-01.txt      \n",
            " extracting: ICSA-19-038-02.txt      \n",
            " extracting: ICSA-19-036-01.txt      \n",
            " extracting: ICSA-19-036-02.txt      \n",
            " extracting: ICSA-19-036-04.txt      \n",
            " extracting: ICSA-19-036-05.txt      \n",
            " extracting: ICSA-19-036-03.txt      \n",
            " extracting: ICSA-19-031-02.txt      \n",
            " extracting: ICSA-19-031-01.txt      \n",
            " extracting: ICSMA-19-029-01.txt     \n",
            " extracting: ICSMA-19-029-02.txt     \n",
            " extracting: ICSA-19-029-01.txt      \n",
            " extracting: ICSA-19-029-02.txt      \n",
            " extracting: ICSA-19-029-03.txt      \n",
            " extracting: ICSA-19-024-01.txt      \n",
            " extracting: ICSA-19-024-02.txt      \n",
            " extracting: ICSMA-19-022-01.txt     \n",
            " extracting: ICSA-19-022-01.txt      \n",
            " extracting: ICSA-19-017-01.txt      \n",
            " extracting: ICSA-19-017-02.txt      \n",
            " extracting: ICSA-19-017-03.txt      \n",
            " extracting: ICSA-19-015-01.txt      \n",
            " extracting: ICSA-19-010-01.txt      \n",
            " extracting: ICSA-19-010-02.txt      \n",
            " extracting: ICSA-19-010-03.txt      \n",
            " extracting: ICSA-18-333-02.txt      \n",
            " extracting: ICSA-19-008-01.txt      \n",
            " extracting: ICSA-19-008-02.txt      \n",
            " extracting: ICSA-19-003-01.txt      \n",
            " extracting: ICSA-19-003-02.txt      \n",
            " extracting: ICSA-19-003-03.txt      \n",
            " extracting: ICSA-18-354-01.txt      \n",
            " extracting: ICSA-18-354-02.txt      \n",
            " extracting: ICSA-18-331-02.txt      \n",
            " extracting: ICSA-18-352-01.txt      \n",
            " extracting: ICSA-18-352-02.txt      \n",
            " extracting: ICSA-18-352-03.txt      \n",
            " extracting: ICSA-18-352-04.txt      \n",
            " extracting: ICSA-18-352-05.txt      \n",
            " extracting: ICSA-18-352-06.txt      \n",
            " extracting: ICSA-18-352-07.txt      \n",
            " extracting: ICSMA-18-347-01.txt     \n",
            " extracting: ICSA-18-347-01.txt      \n",
            " extracting: ICSA-18-347-02.txt      \n",
            " extracting: ICSA-18-347-03.txt      \n",
            " extracting: ICSA-18-347-04.txt      \n",
            " extracting: ICSA-18-345-01.txt      \n",
            " extracting: ICSA-18-345-02.txt      \n",
            " extracting: ICSMA-18-340-01.txt     \n",
            " extracting: ICSA-18-340-01.txt      \n",
            " extracting: ICSA-18-310-02.txt      \n",
            " extracting: ICSA-18-338-01.txt      \n",
            " extracting: ICSA-18-338-02.txt      \n",
            " extracting: ICSA-18-333-01.txt      \n",
            " extracting: ICSA-18-331-01.txt      \n",
            " extracting: ICSA-18-324-01.txt      \n",
            " extracting: ICSA-18-324-02.txt      \n",
            " extracting: ICSA-18-317-01.txt      \n",
            " extracting: ICSA-18-317-02.txt      \n",
            " extracting: ICSA-18-317-03.txt      \n",
            " extracting: ICSA-18-317-04.txt      \n",
            " extracting: ICSA-18-317-05.txt      \n",
            " extracting: ICSA-18-317-06.txt      \n",
            " extracting: ICSA-18-317-07.txt      \n",
            " extracting: ICSA-18-317-08.txt      \n",
            " extracting: ICSMA-18-312-01.txt     \n",
            " extracting: ICSMA-18-310-01.txt     \n",
            " extracting: ICSA-18-305-01.txt      \n",
            " extracting: ICSA-18-305-02.txt      \n",
            " extracting: ICSA-18-305-03.txt      \n",
            " extracting: ICSA-18-305-04.txt      \n",
            " extracting: ICSA-18-303-01.txt      \n",
            " extracting: ICSA-18-298-01.txt      \n",
            " extracting: ICSA-18-298-02.txt      \n",
            " extracting: ICSA-18-296-01.txt      \n",
            " extracting: ICSA-18-296-02.txt      \n",
            " extracting: ICSA-18-296-03.txt      \n",
            " extracting: ICSA-18-290-01.txt      \n",
            " extracting: ICSA-18-289-01.txt      \n",
            " extracting: ICSA-18-284-01.txt      \n",
            " extracting: ICSA-18-284-02.txt      \n",
            " extracting: ICSA-18-284-03.txt      \n",
            " extracting: ICSA-18-282-01.txt      \n",
            " extracting: ICSA-18-282-02.txt      \n",
            " extracting: ICSA-18-282-03.txt      \n",
            " extracting: ICSA-18-282-04.txt      \n",
            " extracting: ICSA-18-282-05.txt      \n",
            " extracting: ICSA-18-282-06.txt      \n",
            " extracting: ICSA-18-282-07.txt      \n",
            " extracting: ICSMA-18-277-01.txt     \n",
            " extracting: ICSMA-18-277-02.txt     \n",
            " extracting: ICSA-18-277-01.txt      \n",
            " extracting: ICSA-18-275-01.txt      \n",
            " extracting: ICSA-18-275-02.txt      \n",
            " extracting: ICSA-18-275-03.txt      \n",
            " extracting: ICSA-18-270-01.txt      \n",
            " extracting: ICSA-18-270-02.txt      \n",
            " extracting: ICSA-18-270-03.txt      \n",
            " extracting: ICSA-18-270-04.txt      \n",
            " extracting: ICSA-18-263-01.txt      \n",
            " extracting: ICSA-18-263-02.txt      \n",
            " extracting: ICSA-18-261-01.txt      \n",
            " extracting: ICSA-18-256-01.txt      \n",
            " extracting: ICSA-18-254-01.txt      \n",
            " extracting: ICSA-18-254-02.txt      \n",
            " extracting: ICSA-18-254-03.txt      \n",
            " extracting: ICSA-18-254-04.txt      \n",
            " extracting: ICSA-18-254-05.txt      \n",
            " extracting: ICSA-18-249-01.txt      \n",
            " extracting: ICSA-18-247-01.txt      \n",
            " extracting: ICSA-18-242-01.txt      \n",
            " extracting: ICSMA-18-240-01.txt     \n",
            " extracting: ICSA-18-240-01.txt      \n",
            " extracting: ICSA-18-240-02.txt      \n",
            " extracting: ICSA-18-240-03.txt      \n",
            " extracting: ICSA-18-240-04.txt      \n",
            " extracting: ICSMA-18-235-01.txt     \n",
            " extracting: ICSMA-18-233-01.txt     \n",
            " extracting: ICSA-18-233-01.txt      \n",
            " extracting: ICSA-18-228-01.txt      \n",
            " extracting: ICSA-18-191-03.txt      \n",
            " extracting: ICSMA-18-226-01.txt     \n",
            " extracting: ICSA-18-226-01.txt      \n",
            " extracting: ICSA-18-226-02.txt      \n",
            " extracting: ICSA-18-226-03.txt      \n",
            " extracting: ICSA-18-221-01.txt      \n",
            " extracting: ICSA-18-221-02.txt      \n",
            " extracting: ICSMA-18-219-01.txt     \n",
            " extracting: ICSMA-18-219-02.txt     \n",
            " extracting: ICSA-18-219-01.txt      \n",
            " extracting: ICSA-18-212-01.txt      \n",
            " extracting: ICSA-18-212-02.txt      \n",
            " extracting: ICSA-18-212-03.txt      \n",
            " extracting: ICSA-18-212-04.txt      \n",
            " extracting: ICSA-18-212-05.txt      \n",
            " extracting: ICSA-18-200-01.txt      \n",
            " extracting: ICSA-18-200-02.txt      \n",
            " extracting: ICSA-18-200-03.txt      \n",
            " extracting: ICSA-18-200-04.txt      \n",
            " extracting: ICSA-18-198-01.txt      \n",
            " extracting: ICSA-18-198-02.txt      \n",
            " extracting: ICSA-18-198-03.txt      \n",
            " extracting: ICSA-18-193-01.txt      \n",
            " extracting: ICSA-18-191-01.txt      \n",
            " extracting: ICSA-18-191-02.txt      \n",
            " extracting: ICSA-18-184-01.txt      \n",
            " extracting: ICSMA-18-179-01.txt     \n",
            " extracting: ICSA-18-172-01.txt      \n",
            " extracting: ICSA-18-172-02.txt      \n",
            " extracting: ICSMA-18-165-01.txt     \n",
            " extracting: ICSA-18-163-01.txt      \n",
            " extracting: ICSA-18-158-01.txt      \n",
            " extracting: ICSMA-18-156-01.txt     \n",
            " extracting: ICSA-18-156-01.txt      \n",
            " extracting: ICSA-18-151-01.txt      \n",
            " extracting: ICSA-18-151-02.txt      \n",
            " extracting: ICSA-18-151-03.txt      \n",
            " extracting: ICSMA-18-144-01.txt     \n",
            " extracting: ICSA-18-144-01.txt      \n",
            " extracting: ICSMA-18-142-01.txt     \n",
            " extracting: ICSA-18-142-01.txt      \n",
            " extracting: ICSMA-18-137-01.txt     \n",
            " extracting: ICSA-18-137-01.txt      \n",
            " extracting: ICSA-18-137-02.txt      \n",
            " extracting: ICSA-18-137-03.txt      \n",
            " extracting: ICSA-18-137-04.txt      \n",
            " extracting: ICSA-18-135-01.txt      \n",
            " extracting: ICSA-18-130-01.txt      \n",
            " extracting: ICSA-18-130-02.txt      \n",
            " extracting: ICSA-18-102-02.txt      \n",
            " extracting: ICSMA-18-128-01.txt     \n",
            " extracting: ICSA-18-128-01.txt      \n",
            " extracting: ICSA-18-128-02.txt      \n",
            " extracting: ICSA-18-128-03.txt      \n",
            " extracting: ICSA-18-123-01.txt      \n",
            " extracting: ICSA-18-116-01.txt      \n",
            " extracting: ICSA-18-116-02.txt      \n",
            " extracting: ICSMA-18-114-01.txt     \n",
            " extracting: ICSA-18-114-01.txt      \n",
            " extracting: ICSA-18-114-02.txt      \n",
            " extracting: ICSA-18-114-03.txt      \n",
            " extracting: ICSA-18-109-01.txt      \n",
            " extracting: ICSMA-18-107-01.txt     \n",
            " extracting: ICSMA-18-107-02.txt     \n",
            " extracting: ICSA-18-107-01.txt      \n",
            " extracting: ICSA-18-107-02.txt      \n",
            " extracting: ICSA-18-107-03.txt      \n",
            " extracting: ICSA-18-107-04.txt      \n",
            " extracting: ICSA-18-107-05.txt      \n",
            " extracting: ICSA-18-102-01.txt      \n",
            " extracting: ICSA-18-100-01.txt      \n",
            " extracting: ICSA-18-100-02.txt      \n",
            " extracting: ICSA-18-095-01.txt      \n",
            " extracting: ICSA-18-095-02.txt      \n",
            " extracting: ICSA-18-095-03.txt      \n",
            " extracting: ICSA-18-093-01.txt      \n",
            " extracting: ICSMA-18-088-01.txt     \n",
            " extracting: ICSA-18-088-01.txt      \n",
            " extracting: ICSA-18-088-02.txt      \n",
            " extracting: ICSA-18-088-03.txt      \n",
            " extracting: ICSA-18-086-01.txt      \n",
            " extracting: ICSMA-18-086-01.txt     \n",
            " extracting: ICSA-18-081-01.txt      \n",
            " extracting: ICSA-18-081-02.txt      \n",
            " extracting: ICSA-18-079-01.txt      \n",
            " extracting: ICSA-18-079-02.txt      \n",
            " extracting: ICSA-18-072-01.txt      \n",
            " extracting: ICSA-18-072-02.txt      \n",
            " extracting: ICSA-18-072-03.txt      \n",
            " extracting: ICSA-18-072-04.txt      \n",
            " extracting: ICSMA-18-037-02.txt     \n",
            " extracting: ICSA-18-067-01.txt      \n",
            " extracting: ICSA-18-067-02.txt      \n",
            " extracting: ICSA-18-065-01.txt      \n",
            " extracting: ICSA-18-065-02.txt      \n",
            " extracting: ICSA-18-065-03.txt      \n",
            " extracting: ICSA-18-060-01.txt      \n",
            " extracting: ICSA-18-060-02.txt      \n",
            " extracting: ICSA-18-060-03.txt      \n",
            " extracting: ICSA-18-058-01A.txt     \n",
            " extracting: ICSA-18-058-02.txt      \n",
            " extracting: ICSA-18-058-03.txt      \n",
            " extracting: ICSMA-18-058-02.txt     \n",
            " extracting: ICSA-18-051-01.txt      \n",
            " extracting: ICSA-18-046-01.txt      \n",
            " extracting: ICSA-18-046-02.txt      \n",
            " extracting: ICSA-18-046-03.txt      \n",
            " extracting: ICSA-18-046-04.txt      \n",
            " extracting: ICSA-18-044-01.txt      \n",
            " extracting: ICSA-18-044-02.txt      \n",
            " extracting: ICSMA-18-037-01.txt     \n",
            " extracting: ICSA-18-032-01.txt      \n",
            " extracting: ICSA-18-032-02.txt      \n",
            " extracting: ICSA-18-032-03.txt      \n",
            " extracting: ICSA-18-030-01.txt      \n",
            " extracting: ICSA-18-030-02.txt      \n",
            " extracting: ICSA-18-025-01.txt      \n",
            " extracting: ICSA-18-025-02B.txt     \n",
            " extracting: ICSMA-18-025-01.txt     \n",
            " extracting: ICSA-18-023-01.txt      \n",
            " extracting: ICSA-18-023-02.txt      \n",
            " extracting: ICSA-18-018-01A.txt     \n",
            " extracting: ICSA-18-011-01.txt      \n",
            " extracting: ICSA-18-011-02.txt      \n",
            " extracting: ICSA-18-011-03.txt      \n",
            " extracting: ICSA-18-009-01.txt      \n",
            " extracting: ICSA-17-234-04.txt      \n",
            " extracting: ICSA-18-004-01.txt      \n",
            " extracting: ICSA-18-004-02A.txt     \n",
            " extracting: ICSA-17-355-01.txt      \n",
            " extracting: ICSA-17-355-02.txt      \n",
            " extracting: ICSA-17-353-01.txt      \n",
            " extracting: ICSA-17-353-02.txt      \n",
            " extracting: ICSA-17-353-03.txt      \n",
            " extracting: ICSA-17-353-04.txt      \n",
            " extracting: ICSA-17-353-05.txt      \n",
            " extracting: ICSA-17-341-01.txt      \n",
            " extracting: ICSA-17-341-02.txt      \n",
            " extracting: ICSA-17-341-03.txt      \n",
            " extracting: ICSA-17-339-01.txt      \n",
            " extracting: ICSA-17-334-01.txt      \n",
            " extracting: ICSA-17-334-02.txt      \n",
            " extracting: ICSA-17-332-01.txt      \n",
            " extracting: ICSMA-17-332-01.txt     \n",
            " extracting: ICSA-17-325-01.txt      \n",
            " extracting: ICSA-17-320-01.txt      \n",
            " extracting: ICSA-17-320-02.txt      \n",
            " extracting: ICSA-17-318-01.txt      \n",
            " extracting: ICSA-17-318-02A.txt     \n",
            " extracting: ICSMA-17-318-01.txt     \n",
            " extracting: ICSA-17-313-01.txt      \n",
            " extracting: ICSA-17-313-02.txt      \n",
            " extracting: ICSA-17-306-01.txt      \n",
            " extracting: ICSA-17-306-02.txt      \n",
            " extracting: ICSA-17-304-01.txt      \n",
            " extracting: ICSA-17-304-02.txt      \n",
            " extracting: ICSA-17-299-01.txt      \n",
            " extracting: ICSA-17-299-02.txt      \n",
            " extracting: ICSA-17-292-01.txt      \n",
            " extracting: ICSMA-17-292-01.txt     \n",
            " extracting: ICSA-17-290-01.txt      \n",
            " extracting: ICSA-17-285-01.txt      \n",
            " extracting: ICSA-17-285-02.txt      \n",
            " extracting: ICSA-17-285-03.txt      \n",
            " extracting: ICSA-17-285-04A.txt     \n",
            " extracting: ICSA-17-285-05.txt      \n",
            " extracting: ICSA-17-283-01.txt      \n",
            " extracting: ICSA-17-283-02.txt      \n",
            " extracting: ICSA-17-278-01A.txt     \n",
            " extracting: ICSA-17-278-02.txt      \n",
            " extracting: ICSA-17-243-01-0.txt    \n",
            " extracting: ICSA-17-271-01B.txt     \n",
            " extracting: ICSA-17-264-01.txt      \n",
            " extracting: ICSA-17-264-02.txt      \n",
            " extracting: ICSA-17-264-03.txt      \n",
            " extracting: ICSA-17-264-04.txt      \n",
            " extracting: ICSA-17-234-05.txt      \n",
            " extracting: ICSA-17-262-01.txt      \n",
            " extracting: ICSA-17-257-01.txt      \n",
            " extracting: ICSA-17-255-01.txt      \n",
            " extracting: ICSMA-17-255-01.txt     \n",
            " extracting: ICSA-17-250-01.txt      \n",
            " extracting: ICSA-17-250-02.txt      \n",
            " extracting: ICSMA-17-250-01.txt     \n",
            " extracting: ICSMA-17-250-02A.txt    \n",
            " extracting: ICSA-17-243-02.txt      \n",
            " extracting: ICSA-17-243-03.txt      \n",
            " extracting: ICSA-17-243-04.txt      \n",
            " extracting: ICSA-17-243-05.txt      \n",
            " extracting: ICSA-17-150-01.txt      \n",
            " extracting: ICSMA-17-241-01.txt     \n",
            " extracting: ICSA-17-241-01.txt      \n",
            " extracting: ICSA-17-241-02.txt      \n",
            " extracting: ICSA-17-236-01.txt      \n",
            " extracting: ICSA-17-208-04.txt      \n",
            " extracting: ICSA-17-234-01.txt      \n",
            " extracting: ICSA-17-234-02.txt      \n",
            " extracting: ICSA-17-234-03.txt      \n",
            " extracting: ICSMA-17-229-01.txt     \n",
            " extracting: ICSA-17-227-01.txt      \n",
            " extracting: ICSMA-17-227-01.txt     \n",
            " extracting: ICSA-17-222-01.txt      \n",
            " extracting: ICSA-17-222-02.txt      \n",
            " extracting: ICSA-17-222-03.txt      \n",
            " extracting: ICSA-17-222-04.txt      \n",
            " extracting: ICSA-17-222-05.txt      \n",
            " extracting: ICSA-17-220-01.txt      \n",
            " extracting: ICSA-17-220-02.txt      \n",
            " extracting: ICSA-17-215-01.txt      \n",
            " extracting: ICSMA-17-215-01.txt     \n",
            " extracting: ICSMA-17-215-02.txt     \n",
            " extracting: ICSA-17-213-01.txt      \n",
            " extracting: ICSA-17-213-02.txt      \n",
            " extracting: ICSA-17-208-01.txt      \n",
            " extracting: ICSA-17-208-02.txt      \n",
            " extracting: ICSA-17-208-03.txt      \n",
            " extracting: ICSA-17-152-02.txt      \n",
            " extracting: ICSA-17-201-01.txt      \n",
            " extracting: ICSA-17-138-03.txt      \n",
            " extracting: ICSA-17-194-01.txt      \n",
            " extracting: ICSA-17-194-03.txt      \n",
            " extracting: ICSA-17-192-01.txt      \n",
            " extracting: ICSA-17-192-02.txt      \n",
            " extracting: ICSA-17-192-03.txt      \n",
            " extracting: ICSA-17-192-04.txt      \n",
            " extracting: ICSA-17-192-05.txt      \n",
            " extracting: ICSA-17-192-06.txt      \n",
            " extracting: ICSA-17-187-01.txt      \n",
            " extracting: ICSA-17-187-02.txt      \n",
            " extracting: ICSA-17-187-03F.txt     \n",
            " extracting: ICSA-17-187-04.txt      \n",
            " extracting: ICSA-17-187-05.txt      \n",
            " extracting: ICSA-17-180-01A.txt     \n",
            " extracting: ICSA-17-180-02.txt      \n",
            " extracting: ICSA-17-180-03.txt      \n",
            " extracting: ICSA-17-178-01.txt      \n",
            " extracting: ICSA-17-173-01.txt      \n",
            " extracting: ICSA-17-173-02.txt      \n",
            " extracting: ICSA-17-171-01.txt      \n",
            " extracting: ICSA-17-166-01.txt      \n",
            " extracting: ICSA-17-164-01.txt      \n",
            " extracting: ICSA-17-164-02.txt      \n",
            " extracting: ICSA-17-164-03.txt      \n",
            " extracting: ICSA-17-157-01.txt      \n",
            " extracting: ICSA-17-157-02.txt      \n",
            " extracting: ICSA-17-152-01.txt      \n",
            " extracting: ICSA-17-143-01.txt      \n",
            " extracting: ICSA-17-115-04.txt      \n",
            " extracting: ICSMA-17-082-02.txt     \n",
            " extracting: ICSA-17-138-01.txt      \n",
            " extracting: ICSA-17-138-02.txt      \n",
            " extracting: ICSA-17-136-01.txt      \n",
            " extracting: ICSA-17-136-02.txt      \n",
            " extracting: ICSA-17-136-03.txt      \n",
            " extracting: ICSA-17-136-04.txt      \n",
            " extracting: ICSA-17-131-01.txt      \n",
            " extracting: ICSA-17-131-02.txt      \n",
            " extracting: ICSA-17-129-01.txt      \n",
            " extracting: ICSA-17-129-02.txt      \n",
            " extracting: ICSA-17-129-03.txt      \n",
            " extracting: ICSA-17-094-04.txt      \n",
            " extracting: ICSA-17-124-01.txt      \n",
            " extracting: ICSA-17-124-02.txt      \n",
            " extracting: ICSA-17-124-03.txt      \n",
            " extracting: ICSA-17-094-05.txt      \n",
            " extracting: ICSA-17-122-01.txt      \n",
            " extracting: ICSA-17-122-02.txt      \n",
            " extracting: ICSA-17-122-03.txt      \n",
            " extracting: ICSA-17-117-01B.txt     \n",
            " extracting: ICSA-17-115-01.txt      \n",
            " extracting: ICSA-17-115-02.txt      \n",
            " extracting: ICSA-17-115-03.txt      \n",
            " extracting: ICSA-17-103-01.txt      \n",
            " extracting: ICSA-17-103-02A.txt     \n",
            " extracting: ICSA-17-101-01.txt      \n",
            " extracting: ICSA-17-096-01A.txt     \n",
            " extracting: ICSA-17-094-01.txt      \n",
            " extracting: ICSA-17-094-02B.txt     \n",
            " extracting: ICSA-17-094-03.txt      \n",
            " extracting: ICSA-17-089-01.txt      \n",
            " extracting: ICSA-17-089-02.txt      \n",
            " extracting: ICSA-17-087-01.txt      \n",
            " extracting: ICSA-17-087-02.txt      \n",
            " extracting: ICSA-17-082-01.txt      \n",
            " extracting: ICSMA-17-082-01.txt     \n",
            " extracting: ICSA-17-047-01.txt      \n",
            " extracting: ICSA-17-047-02.txt      \n",
            " extracting: ICSA-17-075-01.txt      \n",
            " extracting: ICSA-17-073-01.txt      \n",
            " extracting: ICSA-17-068-01.txt      \n",
            " extracting: ICSA-17-066-01.txt      \n",
            " extracting: ICSA-17-061-01.txt      \n",
            " extracting: ICSA-17-061-02.txt      \n",
            " extracting: ICSA-17-061-03.txt      \n",
            " extracting: ICSA-17-059-01.txt      \n",
            " extracting: ICSA-17-054-02.txt      \n",
            " extracting: ICSA-17-054-01.txt      \n",
            " extracting: ICSA-17-054-03.txt      \n",
            " extracting: ICSA-17-045-01.txt      \n",
            " extracting: ICSA-17-045-02.txt      \n",
            " extracting: ICSA-17-045-03.txt      \n",
            " extracting: ICSA-17-040-01.txt      \n",
            " extracting: ICSA-17-038-01.txt      \n",
            " extracting: ICSMA-17-017-01.txt     \n",
            " extracting: ICSMA-17-017-02A.txt    \n",
            " extracting: ICSA-17-033-01.txt      \n",
            " extracting: ICSA-17-031-01A.txt     \n",
            " extracting: ICSA-17-031-02.txt      \n",
            " extracting: ICSA-17-026-01.txt      \n",
            " extracting: ICSA-17-026-02A.txt     \n",
            " extracting: ICSA-17-024-01.txt      \n",
            " extracting: ICSA-17-019-01A.txt     \n",
            " extracting: ICSA-17-017-01.txt      \n",
            " extracting: ICSA-16-336-05B.txt     \n",
            " extracting: ICSA-17-012-01.txt      \n",
            " extracting: ICSA-17-012-02.txt      \n",
            " extracting: ICSA-17-012-03.txt      \n",
            " extracting: ICSA-17-010-01A.txt     \n",
            " extracting: ICSMA-17-009-01A.txt    \n",
            " extracting: ICSA-16-343-05.txt      \n",
            " extracting: ICSA-16-336-06.txt      \n",
            " extracting: ICSA-16-357-01.txt      \n",
            " extracting: ICSA-16-357-02.txt      \n",
            " extracting: ICSA-16-355-01.txt      \n",
            " extracting: ICSA-16-350-01.txt      \n",
            " extracting: ICSA-16-350-02.txt      \n",
            " extracting: ICSA-16-348-01.txt      \n",
            " extracting: ICSA-16-348-02.txt      \n",
            " extracting: ICSA-16-348-03.txt      \n",
            " extracting: ICSA-16-348-04.txt      \n",
            " extracting: ICSA-16-343-01.txt      \n",
            " extracting: ICSA-16-343-02.txt      \n",
            " extracting: ICSA-16-343-03.txt      \n",
            " extracting: ICSA-16-343-04.txt      \n",
            " extracting: ICSA-16-341-01.txt      \n",
            " extracting: ICSA-16-231-01-0.txt    \n",
            " extracting: ICSA-16-336-01.txt      \n",
            " extracting: ICSA-16-336-02.txt      \n",
            " extracting: ICSA-16-336-03.txt      \n",
            " extracting: ICSA-16-336-04.txt      \n",
            " extracting: ICSMA-16-306-01.txt     \n",
            " extracting: ICSA-16-334-01.txt      \n",
            " extracting: ICSA-16-334-02.txt      \n",
            " extracting: ICSA-16-334-03.txt      \n",
            " extracting: ICSA-16-327-01.txt      \n",
            " extracting: ICSA-16-322-01.txt      \n",
            " extracting: ICSA-16-322-02.txt      \n",
            " extracting: ICSA-16-320-01.txt      \n",
            " extracting: ICSA-16-315-01.txt      \n",
            " extracting: ICSA-313-01.txt         \n",
            " extracting: ICS-VU-313-03.txt       \n",
            " extracting: ICSA-16-308-01.txt      \n",
            " extracting: ICSA-16-308-02.txt      \n",
            " extracting: ICSA-16-308-03.txt      \n",
            " extracting: ICSA-16-306-01.txt      \n",
            " extracting: ICSA-16-306-02.txt      \n",
            " extracting: ICSA-16-306-03.txt      \n",
            " extracting: ICSA-16-301-01.txt      \n",
            " extracting: ICSA-16-299-01.txt      \n",
            " extracting: ICSA-16-294-01.txt      \n",
            " extracting: ICSA-16-292-01.txt      \n",
            " extracting: ICSA-16-287-01.txt      \n",
            " extracting: ICSA-16-287-02.txt      \n",
            " extracting: ICSA-16-287-03.txt      \n",
            " extracting: ICSA-16-287-04.txt      \n",
            " extracting: ICSA-16-287-05.txt      \n",
            " extracting: ICSA-16-287-06.txt      \n",
            " extracting: ICSA-16-287-07.txt      \n",
            " extracting: ICSA-16-252-01.txt      \n",
            " extracting: ICSMA-16-279-01.txt     \n",
            " extracting: ICSA-16-278-01.txt      \n",
            " extracting: ICSA-16-278-02.txt      \n",
            " extracting: ICSA-16-273-01-0.txt    \n",
            " extracting: ICSA-16-271-01.txt      \n",
            " extracting: ICSA-16-264-01.txt      \n",
            " extracting: ICSA-16-259-01.txt      \n",
            " extracting: ICSA-16-259-02.txt      \n",
            " extracting: ICSA-16-259-03.txt      \n",
            " extracting: ICSA-16-224-02.txt      \n",
            " extracting: ICSA-16-250-01.txt      \n",
            " extracting: ICSA-16-236-01.txt      \n",
            " extracting: ICSA-16-231-01.txt      \n",
            " extracting: ICSA-16-224-01.txt      \n",
            " extracting: ICSA-16-215-01.txt      \n",
            " extracting: ICSA-16-215-02.txt      \n",
            " extracting: ICSA-16-208-01.txt      \n",
            " extracting: ICSA-16-208-02.txt      \n",
            " extracting: ICSA-16-208-03.txt      \n",
            " extracting: ICSA-16-173-03.txt      \n",
            " extracting: ICSA-16-196-01.txt      \n",
            " extracting: ICSA-16-196-02.txt      \n",
            " extracting: ICSA-16-196-03.txt      \n",
            " extracting: ICSMA-16-196-01.txt     \n",
            " extracting: ICSA-16-194-01.txt      \n",
            " extracting: ICSA-16-194-02.txt      \n",
            " extracting: ICSA-16-189-01.txt      \n",
            " extracting: ICSA-16-189-02.txt      \n",
            " extracting: ICSA-16-187-01.txt      \n",
            " extracting: ICSA-16-182-01.txt      \n",
            " extracting: ICSA-16-182-02.txt      \n",
            " extracting: ICSA-16-175-01.txt      \n",
            " extracting: ICSA-16-175-02.txt      \n",
            " extracting: ICSA-16-175-03.txt      \n",
            " extracting: ICSA-16-173-01.txt      \n",
            " extracting: ICSA-16-173-02.txt      \n",
            " extracting: ICSA-16-168-01.txt      \n",
            " extracting: ICSA-16-166-01.txt      \n",
            " extracting: ICSA-16-166-02.txt      \n",
            " extracting: ICSA-16-161-01.txt      \n",
            " extracting: ICSA-16-161-02.txt      \n",
            " extracting: ICSA-16-159-01.txt      \n",
            " extracting: ICSA-16-126-01.txt      \n",
            " extracting: ICSA-16-154-01.txt      \n",
            " extracting: ICSA-16-152-01.txt      \n",
            " extracting: ICSA-16-152-02.txt      \n",
            " extracting: ICSA-16-147-02.txt      \n",
            " extracting: ICSA-16-147-03.txt      \n",
            " extracting: ICSA-16-147-01.txt      \n",
            " extracting: ICSA-16-145-01.txt      \n",
            " extracting: ICSA-16-140-01.txt      \n",
            " extracting: ICSA-16-140-02.txt      \n",
            " extracting: ICSA-16-138-01.txt      \n",
            " extracting: ICSA-16-042-01.txt      \n",
            " extracting: ICSA-16-133-01.txt      \n",
            " extracting: ICSA-16-131-01.txt      \n",
            " extracting: ICSA-16-105-01.txt      \n",
            " extracting: ICSA-16-105-02.txt      \n",
            " extracting: ICSA-16-105-03.txt      \n",
            " extracting: ICSA-16-103-01.txt      \n",
            " extracting: ICSA-16-103-02.txt      \n",
            " extracting: ICSA-16-103-03.txt      \n",
            " extracting: ICSA-16-070-02.txt      \n",
            " extracting: ICSA-16-096-01.txt      \n",
            " extracting: ICSA-16-061-03.txt      \n",
            " extracting: ICSA-16-056-01.txt      \n",
            " extracting: ICSA-16-091-01.txt      \n",
            " extracting: ICSMA-16-089-01.txt     \n",
            " extracting: ICSA-16-084-01.txt      \n",
            " extracting: ICSA-16-082-01.txt      \n",
            " extracting: ICSA-16-077-01.txt      \n",
            " extracting: ICSA-16-075-01.txt      \n",
            " extracting: ICSA-16-070-01.txt      \n",
            " extracting: ICSA-16-063-01.txt      \n",
            " extracting: ICSA-16-061-01.txt      \n",
            " extracting: ICSA-16-061-02.txt      \n",
            " extracting: ICSA-16-049-01.txt      \n",
            " extracting: ICSA-16-049-02.txt      \n",
            " extracting: ICSA-16-040-01.txt      \n",
            " extracting: ICSA-16-040-02.txt      \n",
            " extracting: ICSA-16-033-01.txt      \n",
            " extracting: ICSA-16-033-02.txt      \n",
            " extracting: ICSA-16-028-01.txt      \n",
            " extracting: ICSA-16-026-01.txt      \n",
            " extracting: ICSA-16-026-02.txt      \n",
            " extracting: ICSA-16-021-01.txt      \n",
            " extracting: ICSA-15-337-02.txt      \n",
            " extracting: ICSA-16-019-01.txt      \n",
            " extracting: ICSA-16-014-01.txt      \n",
            " extracting: ICSA-15-356-01.txt      \n",
            " extracting: ICSA-15-351-01.txt      \n",
            " extracting: ICSA-15-351-02.txt      \n",
            " extracting: ICSA-15-351-03.txt      \n",
            " extracting: ICSA-15-349-01.txt      \n",
            " extracting: ICSA-15-344-01.txt      \n",
            " extracting: ICSA-15-344-02.txt      \n",
            " extracting: ICSA-15-342-01.txt      \n",
            " extracting: ICSA-15-342-02.txt      \n",
            " extracting: ICSA-15-337-03.txt      \n",
            " extracting: ICSA-15-337-01.txt      \n",
            " extracting: ICSA-15-309-02.txt      \n",
            " extracting: ICSA-15-335-01.txt      \n",
            " extracting: ICSA-15-335-02.txt      \n",
            " extracting: ICSA-15-335-03A.txt     \n",
            " extracting: ICSA-15-328-01.txt      \n",
            " extracting: ICSA-15-295-01.txt      \n",
            " extracting: ICSA-15-323-01.txt      \n",
            " extracting: ICSA-15-321-01.txt      \n",
            " extracting: ICSA-15-274-02A.txt     \n",
            " extracting: ICSA-15-309-01.txt      \n",
            " extracting: ICSA-15-300-01.txt      \n",
            " extracting: ICSA-15-300-02A.txt     \n",
            " extracting: ICSA-15-300-03A.txt     \n",
            " extracting: ICSA-15-265-03.txt      \n",
            " extracting: ICSA-15-293-01.txt      \n",
            " extracting: ICSA-15-293-02.txt      \n",
            " extracting: ICSA-15-293-03.txt      \n",
            " extracting: ICSA-15-288-01.txt      \n",
            " extracting: ICSA-15-286-01.txt      \n",
            " extracting: ICSA-15-274-01.txt      \n",
            " extracting: ICSA-15-272-01.txt      \n",
            " extracting: ICSA-15-146-01.txt      \n",
            " extracting: ICSA-15-181-01.txt      \n",
            " extracting: ICSA-15-267-01.txt      \n",
            " extracting: ICSA-15-237-02.txt      \n",
            " extracting: ICSA-15-237-02-Supplement.txt  \n",
            " extracting: ICSA-15-265-01.txt      \n",
            " extracting: ICSA-15-265-02.txt      \n",
            " extracting: ICSA-15-232-01.txt      \n",
            " extracting: ICSA-15-260-01.txt      \n",
            " extracting: ICSA-15-258-01.txt      \n",
            " extracting: ICSA-15-258-02.txt      \n",
            " extracting: ICSA-15-258-03.txt      \n",
            " extracting: ICSA-15-258-04.txt      \n",
            " extracting: ICSA-15-253-01.txt      \n",
            " extracting: ICSA-15-251-01A.txt     \n",
            " extracting: ICSA-15-246-01.txt      \n",
            " extracting: ICSA-15-246-02.txt      \n",
            " extracting: ICSA-15-246-03.txt      \n",
            " extracting: ICSA-15-181-02A.txt     \n",
            " extracting: ICSA-15-244-01.txt      \n",
            " extracting: ICSA-15-239-01.txt      \n",
            " extracting: ICSA-15-239-02.txt      \n",
            " extracting: ICSA-15-239-03.txt      \n",
            " extracting: ICSA-15-237-01.txt      \n",
            " extracting: ICSA-15-225-01.txt      \n",
            " extracting: ICSA-15-223-01.txt      \n",
            " extracting: ICSA-15-211-01.txt      \n",
            " extracting: ICSA-15-202-01.txt      \n",
            " extracting: ICSA-15-202-02.txt      \n",
            " extracting: ICSA-15-202-03B.txt     \n",
            " extracting: ICSA-15-174-01.txt      \n",
            " extracting: ICSA-15-006-01.txt      \n",
            " extracting: ICSA-15-195-01.txt      \n",
            " extracting: ICSA-15-176-01.txt      \n",
            " extracting: ICSA-15-176-02.txt      \n",
            " extracting: ICSA-15-169-01.txt      \n",
            " extracting: ICSA-15-169-02.txt      \n",
            " extracting: ICSA-15-167-01.txt      \n",
            " extracting: ICSA-15-162-01A.txt     \n",
            " extracting: ICSA-15-161-01.txt      \n",
            " extracting: ICSA-15-125-01B.txt     \n",
            " extracting: ICSA-15-160-02.txt      \n",
            " extracting: ICSA-15-160-01.txt      \n",
            " extracting: ICSA-15-155-01.txt      \n",
            " extracting: ICSA-15-153-01.txt      \n",
            " extracting: ICSA-15-153-02.txt      \n",
            " extracting: ICSA-15-148-01.txt      \n",
            " extracting: ICSA-15-132-02.txt      \n",
            " extracting: ICSA-15-141-01A.txt     \n",
            " extracting: ICSA-15-111-01.txt      \n",
            " extracting: ICSA-14-202-01A.txt     \n",
            " extracting: ICSA-15-132-01.txt      \n",
            " extracting: ICSA-15-111-02.txt      \n",
            " extracting: ICSA-15-120-01.txt      \n",
            " extracting: ICSA-15-099-01E.txt     \n",
            " extracting: ICSA-15-064-01A.txt     \n",
            " extracting: ICSA-15-064-02A.txt     \n",
            " extracting: ICSA-15-097-01.txt      \n",
            " extracting: ICSA-15-092-01.txt      \n",
            " extracting: ICSA-15-090-01.txt      \n",
            " extracting: ICSA-15-090-02.txt      \n",
            " extracting: ICSA-15-090-03.txt      \n",
            " extracting: ICSA-15-085-01A.txt     \n",
            " extracting: ICSA-15-036-01A.txt     \n",
            " extracting: ICSA-15-062-02.txt      \n",
            " extracting: ICSA-15-076-01.txt      \n",
            " extracting: ICSA-15-076-02.txt      \n",
            " extracting: ICSA-14-350-02.txt      \n",
            " extracting: ICSA-15-071-01.txt      \n",
            " extracting: ICSA-15-069-04A.txt     \n",
            " extracting: ICSA-15-069-01.txt      \n",
            " extracting: ICSA-15-069-02.txt      \n",
            " extracting: ICSA-15-069-03.txt      \n",
            " extracting: ICSA-15-041-02.txt      \n",
            " extracting: ICSA-15-064-03.txt      \n",
            " extracting: ICSA-15-064-04.txt      \n",
            " extracting: ICSA-15-064-05.txt      \n",
            " extracting: ICSA-14-353-01-SupplementA.txt  \n",
            " extracting: ICSA-15-062-01.txt      \n",
            " extracting: ICSA-15-057-01.txt      \n",
            " extracting: ICSA-15-055-01.txt      \n",
            " extracting: ICSA-15-055-02.txt      \n",
            " extracting: ICSA-15-055-03.txt      \n",
            " extracting: ICSA-15-050-01A.txt     \n",
            " extracting: ICSA-15-048-01.txt      \n",
            " extracting: ICSA-15-048-02.txt      \n",
            " extracting: ICSA-15-048-03.txt      \n",
            " extracting: ICSA-14-198-03G.txt     \n",
            " extracting: ICSA-15-041-01.txt      \n",
            " extracting: ICSA-14-329-02D.txt     \n",
            " extracting: ICSA-15-036-02.txt      \n",
            " extracting: ICSA-15-012-01C.txt     \n",
            " extracting: ICSA-14-353-01C.txt     \n",
            " extracting: ICSA-15-034-01.txt      \n",
            " extracting: ICSA-15-034-02.txt      \n",
            " extracting: ICSA-15-029-01.txt      \n",
            " extracting: ICSA-15-027-01.txt      \n",
            " extracting: ICSA-15-027-02.txt      \n",
            " extracting: ICSA-15-022-01.txt      \n",
            " extracting: ICSA-15-020-01.txt      \n",
            " extracting: ICSA-15-020-02.txt      \n",
            " extracting: ICSA-14-345-01.txt      \n",
            " extracting: ICSA-14-289-02.txt      \n",
            " extracting: ICSA-14-287-01.txt      \n",
            " extracting: ICSA-15-013-01.txt      \n",
            " extracting: ICSA-15-013-02.txt      \n",
            " extracting: ICSA-15-013-03.txt      \n",
            " extracting: ICSA-15-013-04A.txt     \n",
            " extracting: ICSA-15-008-01A.txt     \n",
            " extracting: ICSA-15-008-02.txt      \n",
            " extracting: ICSA-14-352-01.txt      \n",
            " extracting: ICSA-14-352-02.txt      \n",
            " extracting: ICSA-13-259-01B.txt     \n",
            " extracting: ICSA-14-350-01.txt      \n",
            " extracting: ICSA-14-343-01.txt      \n",
            " extracting: ICSA-14-343-02.txt      \n",
            " extracting: ICSA-14-303-02.txt      \n",
            " extracting: ICSA-14-260-01A.txt     \n",
            " extracting: ICSA-14-329-01.txt      \n",
            " extracting: ICSA-14-324-01.txt      \n",
            " extracting: ICSA-14-294-01.txt      \n",
            " extracting: ICSA-14-308-01.txt      \n",
            " extracting: ICSA-14-303-01.txt      \n",
            " extracting: ICSA-14-275-01.txt      \n",
            " extracting: ICSA-14-275-02.txt      \n",
            " extracting: ICSA-14-247-01A.txt     \n",
            " extracting: ICSA-14-135-03A.txt     \n",
            " extracting: ICSA-14-289-01.txt      \n",
            " extracting: ICSA-14-269-02.txt      \n",
            " extracting: ICSA-14-269-01-Supplement.txt  \n",
            " extracting: ICSA-14-269-01A.txt     \n",
            " extracting: ICSA-14-288-01.txt      \n",
            " extracting: ICSA-14-259-01A.txt     \n",
            " extracting: ICSA-14-205-02A.txt     \n",
            " extracting: ICSA-14-273-01.txt      \n",
            " extracting: ICSA-14-254-02.txt      \n",
            " extracting: ICSA-14-261-01.txt      \n",
            " extracting: ICSA-14-254-01.txt      \n",
            " extracting: ICSA-14-224-01.txt      \n",
            " extracting: ICSA-14-238-01.txt      \n",
            " extracting: ICSA-14-238-02.txt      \n",
            " extracting: ICSA-14-226-01.txt      \n",
            " extracting: ICSA-14-196-01.txt      \n",
            " extracting: ICSA-14-189-02.txt      \n",
            " extracting: ICSA-14-205-01.txt      \n",
            " extracting: ICSA-14-007-01B.txt     \n",
            " extracting: ICSA-14-203-01.txt      \n",
            " extracting: ICSA-14-175-01.txt      \n",
            " extracting: ICSA-14-198-02.txt      \n",
            " extracting: ICSA-14-198-01.txt      \n",
            " extracting: ICSA-14-189-01.txt      \n",
            " extracting: ICSA-14-126-01A.txt     \n",
            " extracting: ICSA-14-178-01.txt      \n",
            " extracting: ICSA-14-156-01.txt      \n",
            " extracting: ICSA-14-154-01.txt      \n",
            " extracting: ICSA-14-149-01.txt      \n",
            " extracting: ICSA-14-149-02.txt      \n",
            " extracting: ICSA-14-087-01A.txt     \n",
            " extracting: ICSA-14-051-03B.txt     \n",
            " extracting: ICSA-14-133-02.txt      \n",
            " extracting: ICSA-14-105-03B.txt     \n",
            " extracting: ICSA-14-135-01.txt      \n",
            " extracting: ICSA-14-135-02.txt      \n",
            " extracting: ICSA-14-135-04.txt      \n",
            " extracting: ICSA-14-135-05.txt      \n",
            " extracting: ICSA-14-133-01.txt      \n",
            " extracting: ICSA-14-070-01A.txt     \n",
            " extracting: ICSA-14-128-01.txt      \n",
            " extracting: ICSA-14-121-01.txt      \n",
            " extracting: ICSA-14-091-01.txt      \n",
            " extracting: ICSA-14-114-01.txt      \n",
            " extracting: ICSA-14-114-02.txt      \n",
            " extracting: ICSA-14-084-01.txt      \n",
            " extracting: ICSA-14-107-02.txt      \n",
            " extracting: ICSA-14-107-01.txt      \n",
            " extracting: ICSA-14-105-02A.txt     \n",
            " extracting: ICSA-14-105-01.txt      \n",
            " extracting: ICSA-14-100-01.txt      \n",
            " extracting: ICSA-12-342-01B.txt     \n",
            " extracting: ICSA-13-291-01B.txt     \n",
            " extracting: ICSA-14-098-01.txt      \n",
            " extracting: ICSA-14-098-02.txt      \n",
            " extracting: ICSA-14-098-03.txt      \n",
            " extracting: ICSA-14-079-03.txt      \n",
            " extracting: ICSA-14-093-01.txt      \n",
            " extracting: ICSA-14-086-01A.txt     \n",
            " extracting: ICSA-14-086-01.txt      \n",
            " extracting: ICSA-14-079-01.txt      \n",
            " extracting: ICSA-14-079-02.txt      \n",
            " extracting: ICSA-12-213-01A.txt     \n",
            " extracting: ICSA-14-073-01.txt      \n",
            " extracting: ICSA-14-072-01.txt      \n",
            " extracting: ICSA-14-058-01.txt      \n",
            " extracting: ICSA-14-058-02.txt      \n",
            " extracting: ICSA-13-350-01A.txt     \n",
            " extracting: ICSA-14-051-01.txt      \n",
            " extracting: ICSA-14-051-02.txt      \n",
            " extracting: ICSA-14-051-04.txt      \n",
            " extracting: ICSA-14-010-01.txt      \n",
            " extracting: ICSA-14-035-01.txt      \n",
            " extracting: ICSA-14-021-01.txt      \n",
            " extracting: ICSA-14-030-01.txt      \n",
            " extracting: ICSA-14-006-01.txt      \n",
            " extracting: ICSA-14-023-01.txt      \n",
            " extracting: ICSA-14-016-01.txt      \n",
            " extracting: ICSA-13-344-01.txt      \n",
            " extracting: ICSA-14-014-01.txt      \n",
            " extracting: ICSA-14-008-01.txt      \n",
            " extracting: ICSA-11-094-02B.txt     \n",
            " extracting: ICSA-13-352-01.txt      \n",
            " extracting: ICSA-13-347-01.txt      \n",
            " extracting: ICSA-13-346-01.txt      \n",
            " extracting: ICSA-13-346-02.txt      \n",
            " extracting: ICSA-13-340-01.txt      \n",
            " extracting: ICSA-13-338-01.txt      \n",
            " extracting: ICSA-13-337-01.txt      \n",
            " extracting: ICSA-13-329-01.txt      \n",
            " extracting: ICSA-13-297-01.txt      \n",
            " extracting: ICSA-13-297-02.txt      \n",
            " extracting: ICSA-13-295-01.txt      \n",
            " extracting: ICSA-13-282-01A.txt     \n",
            " extracting: ICSA-13-289-01.txt      \n",
            " extracting: ICSA-13-276-01.txt      \n",
            " extracting: ICSA-13-095-02A.txt     \n",
            " extracting: ICSA-13-277-01.txt      \n",
            " extracting: ICSA-13-274-01.txt      \n",
            " extracting: ICSA-12-018-01B.txt     \n",
            " extracting: ICSA-13-231-01B.txt     \n",
            " extracting: ICSA-13-254-01.txt      \n",
            " extracting: ICSA-13-252-01.txt      \n",
            " extracting: ICSA-13-248-01.txt      \n",
            " extracting: ICSA-13-213-04A.txt     \n",
            " extracting: ICSA-13-240-01.txt      \n",
            " extracting: ICSA-13-234-01.txt      \n",
            " extracting: ICSA-13-234-02.txt      \n",
            " extracting: ICSA-13-233-01.txt      \n",
            " extracting: ICSA-13-226-01.txt      \n",
            " extracting: ICSA-13-225-01.txt      \n",
            " extracting: ICSA-13-225-02.txt      \n",
            " extracting: ICSA-12-228-01A.txt     \n",
            " extracting: ICSA-13-219-01.txt      \n",
            " extracting: ICSA-13-217-02.txt      \n",
            " extracting: ICSA-13-217-01.txt      \n",
            " extracting: ICSA-13-213-01.txt      \n",
            " extracting: ICSA-13-213-02.txt      \n",
            " extracting: ICSA-13-213-03.txt      \n",
            " extracting: ICSA-13-170-01.txt      \n",
            " extracting: ICSA-13-189-01.txt      \n",
            " extracting: ICSA-13-189-02.txt      \n",
            " extracting: ICSA-13-184-01.txt      \n",
            " extracting: ICSA-13-184-02.txt      \n",
            " extracting: ICSA-13-169-01.txt      \n",
            " extracting: ICSA-13-169-02.txt      \n",
            " extracting: ICSA-13-169-03.txt      \n",
            " extracting: ICSA-13-161-01.txt      \n",
            " extracting: ICSA-13-077-01B.txt     \n",
            " extracting: ICSA-13-142-01.txt      \n",
            " extracting: ICSA-13-140-01.txt      \n",
            " extracting: ICSA-13-136-01.txt      \n",
            " extracting: ICSA-13-113-01.txt      \n",
            " extracting: ICSA-12-354-01A.txt     \n",
            " extracting: ICSA-13-106-01.txt      \n",
            " extracting: ICSA-13-116-01.txt      \n",
            " extracting: ICSA-13-100-01.txt      \n",
            " extracting: ICSA-13-098-01.txt      \n",
            " extracting: ICSA-13-095-01.txt      \n",
            " extracting: ICSA-13-091-01.txt      \n",
            " extracting: ICSA-13-050-01A.txt     \n",
            " extracting: ICSA-13-043-02A.txt     \n",
            " extracting: ICSA-13-084-01.txt      \n",
            " extracting: ICSA-13-067-02.txt      \n",
            " extracting: ICSA-13-079-01.txt      \n",
            " extracting: ICSA-13-079-02.txt      \n",
            " extracting: ICSA-13-079-03.txt      \n",
            " extracting: ICSA-13-053-02A.txt     \n",
            " extracting: ICSA-13-067-01.txt      \n",
            " extracting: ICSA-13-053-01.txt      \n",
            " extracting: ICSA-13-038-01A.txt     \n",
            " extracting: ICSA-13-024-01.txt      \n",
            " extracting: ICSA-13-045-01.txt      \n",
            " extracting: ICSA-13-043-01.txt      \n",
            " extracting: ICSA-13-042-01.txt      \n",
            " extracting: ICSA-13-036-02.txt      \n",
            " extracting: ICSA-13-022-02.txt      \n",
            " extracting: ICSA-13-022-01.txt      \n",
            " extracting: ICSA-13-018-01.txt      \n",
            " extracting: ICSA-13-016-01.txt      \n",
            " extracting: ICSA-13-014-01.txt      \n",
            " extracting: ICSA-13-011-02.txt      \n",
            " extracting: ICSA-13-011-03.txt      \n",
            " extracting: ICSA-13-011-01.txt      \n",
            " extracting: ICSA-12-341-01.txt      \n",
            " extracting: ICSA-12-362-01.txt      \n",
            " extracting: ICSA-12-349-01.txt      \n",
            " extracting: ICSA-12-348-01.txt      \n",
            " extracting: ICSA-11-314-01.txt      \n",
            " extracting: ICSA-12-297-01.txt      \n",
            " extracting: ICSA-12-354-02.txt      \n",
            " extracting: ICSA-12-335-01.txt      \n",
            " extracting: ICSA-12-325-01.txt      \n",
            " extracting: ICSA-12-320-01.txt      \n",
            " extracting: ICSA-12-271-01.txt      \n",
            " extracting: ICSA-12-305-01.txt      \n",
            " extracting: ICSA-12-297-02.txt      \n",
            " extracting: ICSA-12-234-01.txt      \n",
            " extracting: ICSA-12-283-01.txt      \n",
            " extracting: ICSA-12-283-02.txt      \n",
            " extracting: ICSA-12-265-01.txt      \n",
            " extracting: ICSA-12-271-02.txt      \n",
            " extracting: ICSA-12-263-02.txt      \n",
            " extracting: ICSA-12-263-01.txt      \n",
            " extracting: ICSA-12-258-01.txt      \n",
            " extracting: ICSA-12-262-01.txt      \n",
            " extracting: ICSA-12-256-01.txt      \n",
            " extracting: ICSA-12-150-01.txt      \n",
            " extracting: ICSA-12-251-01.txt      \n",
            " extracting: ICSA-12-249-01.txt      \n",
            " extracting: ICSA-12-249-02.txt      \n",
            " extracting: ICSA-12-249-03.txt      \n",
            " extracting: ICSA-12-243-01.txt      \n",
            " extracting: ICSA-12-227-01.txt      \n",
            " extracting: ICSA-12-214-01.txt      \n",
            " extracting: ICSA-12-212-02.txt      \n",
            " extracting: ICSA-12-212-01.txt      \n",
            " extracting: ICSA-12-205-02.txt      \n",
            " extracting: ICSA-12-205-01.txt      \n",
            " extracting: ICSA-12-201-01.txt      \n",
            " extracting: ICSA-12-177-02.txt      \n",
            " extracting: ICSA-12-185-01.txt      \n",
            " extracting: ICSA-12-131-02.txt      \n",
            " extracting: ICSA-12-179-01.txt      \n",
            " extracting: ICSA-12-171-01.txt      \n",
            " extracting: ICSA-12-146-01A.txt     \n",
            " extracting: ICSA-12-167-01.txt      \n",
            " extracting: ICSA-12-158-01.txt      \n",
            " extracting: ICSA-12-138-01.txt      \n",
            " extracting: ICSA-12-145-02.txt      \n",
            " extracting: ICSA-12-145-01.txt      \n",
            " extracting: ICSA-12-137-02.txt      \n",
            " extracting: ICSA-12-131-01.txt      \n",
            " extracting: ICSA-12-129-01.txt      \n",
            " extracting: ICSA-12-122-01.txt      \n",
            " extracting: ICSA-12-030-01A.txt     \n",
            " extracting: ICSA-12-095-01A.txt     \n",
            " extracting: ICSA-12-102-02.txt      \n",
            " extracting: ICSA-12-102-01.txt      \n",
            " extracting: ICSA-12-102-03.txt      \n",
            " extracting: ICSA-12-102-05.txt      \n",
            " extracting: ICSA-12-102-04.txt      \n",
            " extracting: ICSA-12-088-01A.txt     \n",
            " extracting: ICSA-12-062-01.txt      \n",
            " extracting: ICSA-12-081-01.txt      \n",
            " extracting: ICSA-12-083-01.txt      \n",
            " extracting: ICSA-12-079-01.txt      \n",
            " extracting: ICSA-12-032-03.txt      \n",
            " extracting: ICSA-12-032-01.txt      \n",
            " extracting: ICSA-12-032-02.txt      \n",
            " extracting: ICSA-12-059-01.txt      \n",
            " extracting: ICSA-12-025-02A.txt     \n",
            " extracting: ICSA-12-047-01A.txt     \n",
            " extracting: ICSA-12-039-01.txt      \n",
            " extracting: ICSA-12-013-01.txt      \n",
            " extracting: ICSA-12-012-01A.txt     \n",
            " extracting: ICSA-12-024-01.txt      \n",
            " extracting: ICSA-12-024-02.txt      \n",
            " extracting: ICSA-12-018-02.txt      \n",
            " extracting: ICSA-12-016-01.txt      \n",
            " extracting: ICSA-11-353-01.txt      \n",
            " extracting: ICSA-12-006-01.txt      \n",
            " extracting: ICSA-11-343-01.txt      \n",
            " extracting: ICSA-11-332-01A.txt     \n",
            " extracting: ICSA-11-362-01.txt      \n",
            " extracting: ICSA-11-361-01.txt      \n",
            " extracting: ICSA-11-298-01A.txt     \n",
            " extracting: ICSA-11-356-01.txt      \n",
            " extracting: ICSA-11-355-01.txt      \n",
            " extracting: ICSA-11-335-01.txt      \n",
            " extracting: ICSA-11-355-02.txt      \n",
            " extracting: ICSA-11-340-01.txt      \n",
            " extracting: ICSA-11-243-03A.txt     \n",
            " extracting: ICSA-11-307-01.txt      \n",
            " extracting: ICSA-11-319-01.txt      \n",
            " extracting: ICSA-11-279-02.txt      \n",
            " extracting: ICSA-11-279-01.txt      \n",
            " extracting: ICSA-11-243-01.txt      \n",
            " extracting: ICSA-11-243-02.txt      \n",
            " extracting: ICSA-11-294-01.txt      \n",
            " extracting: ICSA-11-277-01.txt      \n",
            " extracting: ICSA-11-285-01.txt      \n",
            " extracting: ICSA-11-279-03A.txt     \n",
            " extracting: ICSA-11-280-01.txt      \n",
            " extracting: ICSA-11-279-04.txt      \n",
            " extracting: ICSA-11-273-03A.txt     \n",
            " extracting: ICSA-11-273-01.txt      \n",
            " extracting: ICSA-11-273-02.txt      \n",
            " extracting: ICSA-11-264-01.txt      \n",
            " extracting: ICSA-11-263-01.txt      \n",
            " extracting: ICSA-11-216-01.txt      \n",
            " extracting: ICSA-11-244-01.txt      \n",
            " extracting: ICSA-11-173-01.txt      \n",
            " extracting: ICSA-11-223-01A.txt     \n",
            " extracting: ICSA-11-231-01.txt      \n",
            " extracting: ICSA-11-103-01A.txt     \n",
            " extracting: ICSA-11-195-01.txt      \n",
            " extracting: ICSA-11-189-01.txt      \n",
            " extracting: ICSA-11-182-01.txt      \n",
            " extracting: ICSA-11-182-02.txt      \n",
            " extracting: ICSA-11-175-02.txt      \n",
            " extracting: ICSA-11-168-01A.txt     \n",
            " extracting: ICSA-11-175-01.txt      \n",
            " extracting: ICSA-11-122-01.txt      \n",
            " extracting: ICSA-11-167-01.txt      \n",
            " extracting: ICSA-11-056-01A.txt     \n",
            " extracting: ICSA-11-161-01.txt      \n",
            " extracting: ICSA-11-069-01B.txt     \n",
            " extracting: ICSA-11-132-01A.txt     \n",
            " extracting: ICSA-11-147-01B.txt     \n",
            " extracting: ICSA-11-147-02.txt      \n",
            " extracting: ICSA-11-131-01.txt      \n",
            " extracting: ICSA-11-126-01.txt      \n",
            " extracting: NCCIC-Advisory-May-2011.txt  \n",
            " extracting: ICSA-11-119-01.txt      \n",
            " extracting: ICSA-11-110-01.txt      \n",
            " extracting: ICSA-11-108-01.txt      \n",
            " extracting: ICSA-11-094-01.txt      \n",
            " extracting: ICSA-11-096-01.txt      \n",
            " extracting: NCCIC-Advisory-April-2011.txt  \n",
            " extracting: ICSA-11-091-01A.txt     \n",
            " extracting: ICSA-11-084-01.txt      \n",
            " extracting: ICSA-11-082-01.txt      \n",
            " extracting: ICSA-11-074-01.txt      \n",
            " extracting: ICSA-10-348-01A.txt     \n",
            " extracting: ICSA-10-314-01A.txt     \n",
            " extracting: ICSA-11-041-01A.txt     \n",
            " extracting: ICSA-11-018-02.txt      \n",
            " extracting: ICSA-11-025-01.txt      \n",
            " extracting: ICSA-11-018-01.txt      \n",
            " extracting: ICSA-11-017-01.txt      \n",
            " extracting: ICSA-11-017-02.txt      \n",
            " extracting: ICSA-10-337-01.txt      \n",
            " extracting: ICSA-10-362-01.txt      \n",
            " extracting: ICSA-10-316-01A.txt     \n",
            " extracting: ICSA-10-322-01.txt      \n",
            " extracting: ICSA-10-322-02A.txt     \n",
            " extracting: ICSA-10-301-01A.txt     \n",
            " extracting: ICSA-10-313-01.txt      \n",
            " extracting: ICSA-10-272-01.txt      \n",
            " extracting: ICSA-10-264-01.txt      \n",
            " extracting: ICSA-10-238-01B.txt     \n",
            " extracting: ICSA-10-228-01.txt      \n",
            " extracting: ICSA-10-201-01C.txt     \n",
            " extracting: ICSA-10-214-01.txt      \n",
            " extracting: ICSA-10-147-01.txt      \n",
            " extracting: ICSA-10-070-01A.txt     \n",
            " extracting: ICSA-10-097-01.txt      \n",
            " extracting: ICSA-10-090-01.txt      \n",
            " extracting: ICSA-10-070-02.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WVOk2s31XGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "e107fc80-cef4-41af-aaea-a586f5357925"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBG2vk56ngYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the previous examples of vendors\n",
        "df_vendor = pd.read_csv(\"vendors.csv\",header=None)\n",
        "vendor_list = list(df_vendor[0].dropna())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX_yNkPz25hs",
        "colab_type": "text"
      },
      "source": [
        "### 0.0 Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96GZCDe5Fdn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A functin to read all the lines from pure text advisory report\n",
        "def read_file(file_name):\n",
        "  file_content = []\n",
        "  with open(file_name, \"r\") as f:\n",
        "    read_line = f.readlines()\n",
        "  for i in range(len(read_line)):\n",
        "    if read_line[i].rstrip()=='':\n",
        "      continue\n",
        "    else:\n",
        "      file_content.append(read_line[i].rstrip())\n",
        "  return file_content"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijddoM3_lFSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to judge whether the only word in the list\n",
        "def text_equal(line, lst):\n",
        "  st = re.search('[A-Za-z]',line)\n",
        "  s = 0\n",
        "  if st is not None:\n",
        "    s = st.start()\n",
        "  line = line[s:]\n",
        "  for i in lst:\n",
        "    if line.upper() == i:\n",
        "      return 1\n",
        "  return 0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqdkOamyxMWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to see whether the line is a divider for vulneralbility section\n",
        "def check_line(j):\n",
        "  for i in j.split():\n",
        "    if i == 'of':\n",
        "      continue\n",
        "    elif i == 'for':\n",
        "      continue\n",
        "    elif '---------' in i:\n",
        "      return 0\n",
        "    elif i == i.title():\n",
        "      continue\n",
        "    elif i == i.upper():\n",
        "      continue\n",
        "    elif '-' in i:\n",
        "      continue\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  if j[-1] == '.':\n",
        "    return 0\n",
        "  elif len(j) <= 4:\n",
        "    return 0\n",
        "  elif len(j.split())<=4:\n",
        "    return 1\n",
        "\n",
        "  return 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MApKW3OMFn-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to check whether a string contains word(s) from a list\n",
        "def find_sub(string, lst, find_all = 0):\n",
        "  # all the keywords should be in the list\n",
        "  if find_all:\n",
        "    count=0\n",
        "    for i in lst:\n",
        "      if i in string:\n",
        "        count+=1\n",
        "    if count==len(lst):\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  # only one of the keywords in the string would be enough\n",
        "  else:\n",
        "    for i in lst:\n",
        "      if i in string:\n",
        "        return 1\n",
        "    return 0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32rgEKArEdVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to locate the position of certain regex\n",
        "def find_key(line,kwds):\n",
        "  m_min = len(line)\n",
        "  for i in kwds:\n",
        "    m = re.search(r'{}'.format(i), line)\n",
        "    if m is not None:\n",
        "      m = m.start()\n",
        "      # get the smallest position of keywords\n",
        "      if m < m_min:\n",
        "        m_min = m\n",
        "  return m_min"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCnOcUX70urG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to strip noise from a string\n",
        "def polish(text):\n",
        "  text = text.strip()\n",
        "  text = text.strip('.')\n",
        "  text = text.strip(',')\n",
        "  text = text.strip('•')\n",
        "  text = text.strip(':')\n",
        "  text = text.strip(';')\n",
        "  text = text.strip('–')\n",
        "  text = text.strip('-')\n",
        "  text = text.strip('/')\n",
        "  text = re.sub('( |,)?and$','',text)\n",
        "  text = re.sub('( |,)?with ?$','',text)\n",
        "  text = re.sub(' product versions$','',text)\n",
        "  for i in ['\\) and',', and']:\n",
        "    if re.search(i,text) is not None:\n",
        "      if re.search(i,text).end() == len(text):\n",
        "        t = re.search(i,text).start()\n",
        "        text = text[:t]\n",
        "  text = text.rstrip('(')\n",
        "  text = text.rstrip('[')\n",
        "  text = text.lstrip(')')\n",
        "  text = text.lstrip(']')\n",
        "  text = re.sub(',,',',',text)\n",
        "  if re.search('\\(',text) is not None:\n",
        "    if not re.search('\\(',text).start():\n",
        "      text = text.lstrip('(')\n",
        "      text = text.rstrip(')')\n",
        "  if text[:2] == 'll':\n",
        "    text = 'a'+text\n",
        "  return text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwMEgg7z4-0R",
        "colab_type": "text"
      },
      "source": [
        "### 0.1 Text Reading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKZSrUID5e9b",
        "colab_type": "text"
      },
      "source": [
        "#### 0.1.1 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTFKYXAX6CXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilities for text_reading\n",
        "affect_prod_end = ['impact','overview','vulnerability','cwe-538','background']\n",
        "affect_prod_skip = ['Begin Update','End Update']\n",
        "feed_nxt = ['(incl.','incl.','Incl.','Incl','LOGO!','Cat.']\n",
        "feed_nxt_incl = ['(incl.','incl.']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpCmuHtH6heD",
        "colab_type": "text"
      },
      "source": [
        "#### 0.1.2 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMueKnwdDVWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to collect lines in the affected products section\n",
        "def collect_lines(i):\n",
        "  \n",
        "  content = []\n",
        "  tok_content = []\n",
        "  add_line_flag = 0\n",
        "  af_flag = 0\n",
        "\n",
        "  file_content = read_file(i)\n",
        "  noise = '\\. \\. \\. -'\n",
        "\n",
        "  for j in file_content:\n",
        "    if '--------' in j:\n",
        "      continue\n",
        "    j = re.sub(noise,'',j)\n",
        "    # stop if the ending line occurs \n",
        "    if j.split()[-1].lower() in affect_prod_end and add_line_flag:\n",
        "      if j[0]!='·':\n",
        "        add_line_flag = 0  \n",
        "    # read and store the line if the first divider has been identified            \n",
        "    if add_line_flag and not find_sub(j, affect_prod_skip):\n",
        "      tok_sent = tokenize.sent_tokenize(j)\n",
        "      tok_content = tok_content + tok_sent\n",
        "    # initiate reading when the first line occurs\n",
        "    if 'AFFECTED PRODUCT'.upper() in re.sub(r'\\xa0',' ',j) and not af_flag:\n",
        "      add_line_flag = 1\n",
        "      af_flag = 1\n",
        "    if 'AFFECTED PRODUCT'.title() in re.sub(r'\\xa0',' ',j) and not af_flag:\n",
        "      add_line_flag = 1 \n",
        "      af_flag = 1\n",
        "  \n",
        "  # filter and complement sentences that are mistakenly split\n",
        "  num = 0\n",
        "  while num < len(tok_content):\n",
        "    sent = tok_content[num]\n",
        "    # abandon sentences with only symbols\n",
        "    if re.sub('[A-Za-z0-9]','',sent) == sent:\n",
        "      num+=1\n",
        "      continue\n",
        "    # connect tokens to rebuild split sentences\n",
        "    elif re.sub('[A-Za-z]','',sent) == sent:\n",
        "      up = re.sub('[A-Za-z]','',content[-1]) != content[-1]\n",
        "      if num < len(tok_content)-1:\n",
        "        low = re.sub('[A-Za-z]','',tok_content[num+1]) != tok_content[num+1]\n",
        "      else:\n",
        "        low = 1\n",
        "      if up and low:\n",
        "        content[-1] = content[-1] + sent\n",
        "      else:\n",
        "        content.append(sent)\n",
        "    elif find_sub(sent.split()[-1],feed_nxt):\n",
        "      num += 1\n",
        "      lost_part = tok_content[num]\n",
        "      while find_sub(lost_part.split()[-1],feed_nxt_incl):\n",
        "        sent = sent + re.sub('·',' ',lost_part)\n",
        "        num += 1\n",
        "        lost_part = tok_content[num]\n",
        "      sent = sent + re.sub('·',' ',lost_part)\n",
        "      content.append(sent)\n",
        "    elif sent.split()[-1]=='Controllers':\n",
        "      num += 1\n",
        "      lost_part = tok_content[num]\n",
        "      while '·Series' == lost_part.split()[0]:\n",
        "        temp = sent + re.sub('·',' ',lost_part)\n",
        "        content.append(temp)\n",
        "        num += 1\n",
        "        if num == len(tok_content):\n",
        "            num += 1\n",
        "            break\n",
        "        lost_part = tok_content[num]\n",
        "      num -= 1\n",
        "    elif sent.split()[-1]=='with':\n",
        "      flag = 1\n",
        "      num += 1\n",
        "      lost_part = tok_content[num]\n",
        "      while 'with' not in lost_part.split()[-1]:\n",
        "        if get_div(lost_part) == 1:\n",
        "          content.append(lost_part)\n",
        "          num += 1\n",
        "        else:\n",
        "          lost_part = lost_part.split(' are included ')[0]\n",
        "          temp = sent + re.sub('·',' ',lost_part)\n",
        "          content.append(temp)\n",
        "          num += 1\n",
        "        if num == len(tok_content):\n",
        "          num += 1\n",
        "          break\n",
        "        lost_part = tok_content[num]\n",
        "      num -= 1\n",
        "    else:\n",
        "      content.append(sent)\n",
        "    num += 1\n",
        "\n",
        "  return [i, content]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FmaeNRpo964",
        "colab_type": "text"
      },
      "source": [
        "# 1 General Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT-KXXI7pHpE",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cDQ6VVpqalG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilities for getting vendor information\n",
        "new = ['Rockwell','Yokogawa','3S CODESYS','Wonderware','OpenSSL',\n",
        "       'Certec','Sauter','Motorola','IniNet','SchneiderWEB','St. Jude',\n",
        "       'IBHsoftec','DNP3','Everest','eWON','Janitza UMG Power','Tollgrade',\n",
        "       'B. Braun Medical SpaceCom','B+B SmartWorx','PACTware','Kepware',\n",
        "       'Post Oak','Endress+Hauser','Pepperl+Fuchs','Fox','NTP','Cisco',\n",
        "       'Triangle Research','Canary Labs Inc','Trane Tracer SC','Top Server',\n",
        "       'B+B SmartWorx','Pro-Face','Cimon','BMC Medical and 3B Medical',\n",
        "       'BD','RealFlex']\n",
        "vul_sec = ['VULNERABILITY OVERVIEW','VUNLNERABILITY OVERVIEW',\n",
        "           'VULNERABILITY CHARACTERIZATION','VULNERABILITY',\n",
        "           'FILE AND DIRECTORY INFORMATION EXPOSURE CWE-538']\n",
        "vul_gap = ['VULNERABILITY DETAILS','BACKGROUND','RESEARCHER','MITIGATION',\n",
        "           'VUNLNERABILITY DETAILS']\n",
        "vul_err = ['3.3 BACKGROUND','VULNERABILITY OVERVIEW','VULNERABILIY OVERVIEW']\n",
        "# utilities for side cases\n",
        "nt_gap = ['VULNERABILITY DETAILS','BACKGROUND','RESEARCHER','MITIGATION',\n",
        "          'VUNLNERABILITY DETAILS']\n",
        "nt_sec = ['VULNERABILITY OVERVIEW','VUNLNERABILITY OVERVIEW',\n",
        "          'VULNERABILITY CHARACTERIZATION']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnoYNoLbpCf2",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Get Vendors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc_jeBUCqeW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to get the vendor corresponding to the advisory\n",
        "def get_vendor(i):\n",
        "  vendor = ''\n",
        "  for j in read_file(i):\n",
        "    j = re.sub('·','',j)\n",
        "    if 'vendor' in j.lower():\n",
        "      if not re.search('vendor', j.lower()).start():\n",
        "        vd = re.search('vendor', j.lower()).end()\n",
        "        vendor = j[vd:]\n",
        "        vendor = vendor.strip(':')\n",
        "        vendor = vendor.strip()\n",
        "  if vendor == '':\n",
        "    pot_vendor = read_file(i)[1]\n",
        "    for j in vendor_list:\n",
        "      if re.search(j.lower(),pot_vendor.lower()) is not None:\n",
        "        vendor = j\n",
        "  if vendor == '':\n",
        "    pot_vendor = read_file(i)[1]\n",
        "    for j in new:\n",
        "      if j.lower() in pot_vendor.lower():\n",
        "        vendor = j\n",
        "        \n",
        "  return vendor"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtFKHudCqiS8",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Get the Vulnerability Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEg_aggwqlkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to initiate the information of a vulnerability\n",
        "def init():\n",
        "  output = {}\n",
        "  output['Type'] = ''\n",
        "  output['CWE'] = ''\n",
        "  output['CVE'] = ''\n",
        "  output['CVSS'] = {}\n",
        "  output['CVSS']['version'] = ''\n",
        "  output['CVSS']['base score'] = ''\n",
        "  output['CVSS']['temporal score'] = ''\n",
        "  output['CVSS']['vector'] = ''\n",
        "  output['Intrusion'] = ''\n",
        "  return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy-3wZzoqnIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to find the footnotes in a line\n",
        "def find_ft(t,i):\n",
        "  pt = '·'+t+'\\.'\n",
        "  for j in read_file(i):\n",
        "    if re.search(r'{}'.format(pt),j) is not None:\n",
        "      if not re.search(r'{}'.format(pt),j).start():\n",
        "        cwe = find_cwe(i,j)\n",
        "        return cwe"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKHTw8IXqo8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to find the CWE information in a line\n",
        "def find_cwe(f,i):\n",
        "  if \"CWE-\" in i:\n",
        "    v = re.search(r\"CWE-[0-9]{1,}\",i)\n",
        "    if v is None:\n",
        "      v = re.search(r\"CWE- [0-9]{1,}\",i)\n",
        "    cwe = v.group()\n",
        "  elif '[Footnote' in i:\n",
        "    s = re.search('\\[Footnote:',i).end()\n",
        "    ft = i[s:s+2].rstrip(']')\n",
        "    ft = ft.rstrip(',')\n",
        "    cwe=find_ft(ft,f)\n",
        "  else:\n",
        "    cwe=''\n",
        "  return cwe"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smpsv0N2qqYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to find the CVSS version in a line\n",
        "def find_cvss(f):\n",
        "  ver = ''\n",
        "  for i in read_file(f):\n",
        "    if \"CVSS\" in i:\n",
        "      if re.search('CVSS V[0-9] [0-9]\\.[0-9]', i.upper()) is not None:\n",
        "        ver = re.search('CVSS V[0-9] [0-9]\\.[0-9]', i.upper()).group()\n",
        "        ver = re.sub('CVSS','',ver).strip()\n",
        "        return ver\n",
        "      elif re.search('CVSS V[0-9]', i.upper()) is not None:\n",
        "        ver = re.search('CVSS V[0-9]', i.upper()).group()\n",
        "        ver = re.sub('CVSS','',ver).strip()\n",
        "        return ver\n",
        "      elif re.search('CVSSVersion [0-9].[0-9]', i) is not None:\n",
        "        ver = re.search('CVSSVersion [0-9].[0-9]', i).group()\n",
        "        ver = re.sub('CVSS','',ver).strip()\n",
        "        return ver\n",
        "      elif re.search('CVSS Version [0-9].[0-9]', i) is not None:\n",
        "        ver = re.search('CVSS Version [0-9].[0-9]', i).group()\n",
        "        ver = re.sub('CVSS ','',ver).strip()\n",
        "        return ver\n",
        "      elif re.search('CVSSV[0-9]', i.upper()) is not None:\n",
        "        ver = re.search('CVSSV[0-9]', i.upper()).group()\n",
        "        ver = re.sub('CVSS','',ver).strip()\n",
        "        return ver\n",
        "      elif re.search('CVSS:[0-9]\\.[0-9]', i.upper()) is not None:\n",
        "        ver = re.search('CVSS:[0-9]\\.[0-9]', i.upper()).group()\n",
        "        ver = re.sub('CVSS:','',ver).strip()\n",
        "        return ver\n",
        "      elif re.search('CVSSd Version [0-9].[0-9]', i) is not None:\n",
        "        ver = re.search('CVSSd Version [0-9].[0-9]', i).group()\n",
        "        ver = re.sub('CVSSd ','',ver).strip()\n",
        "        return ver\n",
        "  return ver"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVEbFP_0qrkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to find vulnerability informatin in the section\n",
        "def process_vul(i, vul):\n",
        "  # find the CVSS vector\n",
        "  if re.search('([A-Za-z]+:[A-Za-z]+/?){2,}', i) is not None:\n",
        "    vec = re.search('([A-Za-z]+:[A-Za-z]+/?){2,}', i).group()\n",
        "    vul['CVSS']['vector'] = vec\n",
        "  # find the CVE information\n",
        "  if \"CVE-\" in i:\n",
        "    t = re.sub(' ','',i)\n",
        "    vul['CVE'] = re.search(r\"CVE-[0-9]{4}-[0-9]{4,}\",t).group()\n",
        "  # find the CWE information\n",
        "  if \"CWE-\" in i:\n",
        "    v = re.search(r\"CWE-[0-9]{1,}\",i)\n",
        "    if v is None:\n",
        "      v = re.search(r\"CWE- [0-9]{1,}\",i)\n",
        "    vul['CWE'] = v.group()\n",
        "  # find the CVSS version\n",
        "  if \"CVSS\" in i:\n",
        "    if re.search('CVSS V[0-9] [0-9]\\.[0-9]', i.upper()) is not None:\n",
        "      ver = re.search('CVSS V[0-9] [0-9]\\.[0-9]', i.upper()).group()\n",
        "      ver = re.sub('CVSS','',ver).strip()\n",
        "      vul['CVSS']['version'] = ver\n",
        "    elif re.search('CVSS V[0-9]', i.upper()) is not None:\n",
        "      ver = re.search('CVSS V[0-9]', i.upper()).group()\n",
        "      ver = re.sub('CVSS','',ver).strip()\n",
        "      vul['CVSS']['version'] = ver\n",
        "    elif re.search('CVSSVersion [0-9].[0-9]', i) is not None:\n",
        "      ver = re.search('CVSSVersion [0-9].[0-9]', i).group()\n",
        "      ver = re.sub('CVSS','',ver).strip()\n",
        "      vul['CVSS']['version'] = ver\n",
        "    elif re.search('CVSS Version [0-9].[0-9]', i) is not None:\n",
        "      ver = re.search('CVSS Version [0-9].[0-9]', i).group()\n",
        "      ver = re.sub('CVSS ','',ver).strip()\n",
        "      vul['CVSS']['version'] = ver\n",
        "    elif re.search('CVSSV[0-9]', i.upper()) is not None:\n",
        "      ver = re.search('CVSSV[0-9]', i.upper()).group()\n",
        "      ver = re.sub('CVSS','',ver).strip()\n",
        "      vul['CVSS']['version'] = ver\n",
        "    elif re.search('CVSS:[0-9]\\.[0-9]', i.upper()) is not None:\n",
        "      ver = re.search('CVSS:[0-9]\\.[0-9]', i.upper()).group()\n",
        "      ver = re.sub('CVSS:','',ver).strip()\n",
        "      vul['CVSS']['version'] = ver\n",
        "    elif re.search('CVSSd Version [0-9].[0-9]', i) is not None:\n",
        "      ver = re.search('CVSSd Version [0-9].[0-9]', i).group()\n",
        "      ver = re.sub('CVSSd ','',ver).strip()\n",
        "      vul['CVSS']['version'] = ver\n",
        "  # find the base score and temporal score\n",
        "  if 'base score of ' in i:\n",
        "    if re.search('base score of [0-9]{1,}\\.[0-9]{1,}',i) is not None:\n",
        "      s = re.search('base score of [0-9]{1,}\\.[0-9]{1,}',i).group()\n",
        "      s = re.sub('base score of ','',s)\n",
        "      vul['CVSS']['base score'] = s\n",
        "  if \"temporal score of \" in i:\n",
        "    if re.search('temporal score of [0-9]{1,}\\.[0-9]{1,}',i) is not None:\n",
        "      s = re.search('temporal score of [0-9]{1,}\\.[0-9]{1,}',i).group()\n",
        "      s = re.sub('temporal score of ','',s)\n",
        "      vul['CVSS']['temporal score'] = s\n",
        "  # find the intrusions\n",
        "  intru = '(result(s|ing)? in|causes?|allows?( attakers?)?) [A-Za-z ]+(,|.)'\n",
        "  for s in i.split('.'):\n",
        "    if re.search(intru, s) is not None:\n",
        "      vul['Intrusion'] = s\n",
        "  return vul"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9VxGW7Aqu2A",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Get the Vulnerability Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYTNCTptqyWl",
        "colab_type": "text"
      },
      "source": [
        "#### 1.4.1 Side Cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3b1wUoZrAl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to extract vulneralbility information in a side case\n",
        "def side_divider(i,cvss):\n",
        "  flag = 0\n",
        "  x = 0\n",
        "  act = 0\n",
        "  y = 0\n",
        "  line_flag = 0\n",
        "  begin = 0\n",
        "  Vuls=[]\n",
        "  vul = init()\n",
        "  for j in read_file(i):\n",
        "    # stop searching when the end divider appears\n",
        "    gap = ['VULNERABILITY CHARACTERIZATION']\n",
        "    if text_equal(j.upper(),gap):\n",
        "      flag = 0\n",
        "      if vul['CVSS']['version'] == '':\n",
        "        vul['CVSS']['version'] = cvss\n",
        "      try:\n",
        "        vul['text'] = vul_text\n",
        "      except:\n",
        "        vul['text'] = ''\n",
        "      Vuls.append(vul)\n",
        "    if line_flag:\n",
        "      begin = 1\n",
        "      vul = init()\n",
        "      vul['Type'] = re.sub(' CWE-[0-9]{3}$','',ty)\n",
        "      vul['CWE'] = cwe\n",
        "      line_flag = 0\n",
        "    if flag:\n",
        "      if j.upper() == 'VULNERABILITY OVERVIEW':\n",
        "        continue        \n",
        "      s = find_key(j,['\\['])\n",
        "      line = j[:s]\n",
        "      # find the vulnerability type\n",
        "      if check_line(line) and not find_sub(line,[': ']):\n",
        "        try:\n",
        "          complete_vul_text = vul_text\n",
        "        except:\n",
        "          pass\n",
        "        vul_text = line\n",
        "        line_flag = 1\n",
        "        st = re.search('[A-Za-z]',line)\n",
        "        s = 0\n",
        "        if st is not None:\n",
        "          s = st.start()\n",
        "        ty = line[s:]\n",
        "        cwe = find_cwe(i,j)\n",
        "        if x:\n",
        "          if vul['CVSS']['version'] == '':\n",
        "            vul['CVSS']['version'] = cvss\n",
        "          vul['text'] = complete_vul_text\n",
        "          Vuls.append(vul)\n",
        "        x = 1\n",
        "      elif len(line.split())<=4 and not find_sub(line,[': ']):\n",
        "        vul_text = line\n",
        "        line_flag = 1\n",
        "        st = re.search('[A-Za-z]',line)\n",
        "        s = 0\n",
        "        if st is not None:\n",
        "          s = st.start()\n",
        "        ty = line[s:]\n",
        "        cwe = find_cwe(i,j)\n",
        "        if x:\n",
        "          if vul['CVSS']['version'] == '':\n",
        "            vul['CVSS']['version'] = cvss\n",
        "          vul['text'] = complete_vul_text\n",
        "          Vuls.append(vul)\n",
        "        x = 1\n",
        "      elif begin:\n",
        "        vul_text = vul_text + ', ' + j\n",
        "        vul = process_vul(j,vul)\n",
        "    # start searching when the start divider occurs\n",
        "    sec = ['BACKGROUND']\n",
        "    if find_sub(j.upper(),sec):\n",
        "      act = 1\n",
        "      flag = 1  \n",
        "      y = 1\n",
        "  return [x, Vuls]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY4GvQbtrCvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to extract vulneralbility information in anotehr side case\n",
        "def new_side_divider(i,cvss):\n",
        "  \n",
        "  x = 0\n",
        "  flag = 0\n",
        "  act = 0\n",
        "  y = 0 \n",
        "  line_flag = 0\n",
        "  begin = 0\n",
        "  Vuls=[]\n",
        "  vul = init()\n",
        "\n",
        "  for j in read_file(i):  \n",
        "    # stop searching when the ending divider occurs\n",
        "    if j.title() == 'Exploitability':\n",
        "      if vul['CVSS']['version'] == '':\n",
        "        vul['CVSS']['version'] = cvss\n",
        "      try:\n",
        "        vul['text'] = vul_text\n",
        "      except:\n",
        "        vul['text'] = ''\n",
        "      Vuls.append(vul)\n",
        "      flag = 0\n",
        "    # record when the first divider occurs\n",
        "    if line_flag:\n",
        "      begin = 1\n",
        "      vul = init()\n",
        "      vul['Type'] = re.sub(' CWE-[0-9]{3}$','',ty)\n",
        "      vul['CWE'] = cwe\n",
        "      line_flag = 0\n",
        "    if flag:\n",
        "      # skip the wrongly aligned line\n",
        "      if j.upper() == 'VULNERABILITY OVERVIEW':\n",
        "        continue\n",
        "      s = find_key(j,['\\['])\n",
        "      line = j[:s]\n",
        "      # find the subsection in vulneralbility details\n",
        "      if check_line(line):\n",
        "        try:\n",
        "          complete_vul_text = vul_text\n",
        "        except:\n",
        "          pass\n",
        "        vul_text = line\n",
        "        line_flag = 1\n",
        "        st = re.search('[A-Za-z]',line)\n",
        "        s = 0\n",
        "        if st is not None:\n",
        "          s = st.start()\n",
        "        ty = line[s:]\n",
        "        cwe = find_cwe(i,j)\n",
        "        if x:\n",
        "          if vul['CVSS']['version'] == '':\n",
        "            vul['CVSS']['version'] = cvss\n",
        "          vul['text'] = complete_vul_text\n",
        "          Vuls.append(vul)\n",
        "        x = 1\n",
        "      elif begin:\n",
        "        vul_text = vul_text + ', ' + j\n",
        "        vul = process_vul(j,vul)\n",
        "    if j.upper() == 'VULNERABILITY DETAILS':\n",
        "      act = 1\n",
        "      flag = 1 \n",
        "\n",
        "  return [x, Vuls]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahzbwPHXrD-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def no_type(i,cvss):\n",
        "\n",
        "  x = 0\n",
        "  flag = 0\n",
        "  act = 0\n",
        "  begin = 0\n",
        "  line_flag = 1\n",
        "  Vuls=[]\n",
        "  vul = init()\n",
        "\n",
        "  for j in read_file(i):\n",
        "    # stop searching when end divider occurs\n",
        "    if find_sub(j.upper(),nt_gap):\n",
        "      Vuls.append(vul)\n",
        "      flag = 0\n",
        "    if flag:\n",
        "      if j.upper() == 'VULNERABILITY OVERVIEW':\n",
        "        continue\n",
        "      s = find_key(j,['\\['])\n",
        "      line = j[:s]\n",
        "      if check_line(line):\n",
        "        try:\n",
        "          complete_vul_text = vul_text\n",
        "        except:\n",
        "          pass\n",
        "        vul_text = line\n",
        "        x = 1\n",
        "        vul['Type'] = re.sub(' CWE-[0-9]{3}$','',line)\n",
        "      else:\n",
        "        try:\n",
        "          vul_text = vul_text + ', ' + line\n",
        "        except:\n",
        "          vul_text = line\n",
        "        vul = process_vul(line,vul)\n",
        "    # start searching when start divider occurs\n",
        "    if find_sub(j.upper(),nt_sec):\n",
        "      act = 1\n",
        "      flag = 1  \n",
        "      y = 1\n",
        "  try:\n",
        "    vul['text'] = vul_text\n",
        "  except:\n",
        "    vul['text'] = ''\n",
        "  Vuls.append(vul)\n",
        "  return [x, Vuls]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfrZsvNTrFQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def side_case_vul(i):\n",
        "  # get the cvss score from the advisory as backup\n",
        "  cvss = find_cvss(i)\n",
        "  # process three kinds of side cases of vulneralbility\n",
        "  result = side_divider(i,cvss)\n",
        "  if not result[0]:\n",
        "    result = new_side_divider(i,cvss)\n",
        "    if not result[0]:\n",
        "      result = no_type(i,cvss)\n",
        "      return result[1]\n",
        "    else:\n",
        "      return result[1]\n",
        "  else:\n",
        "    return result[1]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRuSS8Quq0Yi",
        "colab_type": "text"
      },
      "source": [
        "#### 1.4.2 General cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTzXZ3rbrHYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vulner(i):\n",
        "  # back up the cvss score\n",
        "  cvss = find_cvss(i)\n",
        "\n",
        "  flag = 0\n",
        "  x = 0\n",
        "  act = 0\n",
        "  y = 0\n",
        "  line_flag = 0\n",
        "  begin = 0\n",
        "\n",
        "  Vuls = []\n",
        "  vul = init()\n",
        "\n",
        "  for j in read_file(i):\n",
        "    # skip when the next line is wrongly placed\n",
        "    if act:\n",
        "      act = 0\n",
        "      if text_equal(j.upper(),vul_err):\n",
        "        continue\n",
        "    # stop collecting when the end occurs\n",
        "    if (text_equal(j.upper(),vul_gap) or j == 'Exploitability') and flag:\n",
        "      flag = 0\n",
        "      if vul['CVSS']['version'] == '':\n",
        "        vul['CVSS']['version'] = cvss\n",
        "      try:\n",
        "        vul['text'] = vul_text\n",
        "      except:\n",
        "        vul['text'] = ''\n",
        "      Vuls.append(vul)\n",
        "    # record when collection is initiated\n",
        "    if line_flag:\n",
        "      begin = 1\n",
        "      vul = init()\n",
        "      vul['Type'] = re.sub(' CWE-[0-9]{3}$','',ty)\n",
        "      vul['CWE'] = cwe\n",
        "      line_flag = 0\n",
        "    if flag:\n",
        "      s = find_key(j,['\\['])\n",
        "      line = j[:s]\n",
        "      if check_line(line):\n",
        "        try:\n",
        "          complete_vul_text = vul_text\n",
        "        except:\n",
        "          pass\n",
        "        vul_text = line\n",
        "        line_flag = 1\n",
        "        st = re.search('[A-Za-z]',line)\n",
        "        s = 0\n",
        "        if st is not None:\n",
        "          s = st.start()\n",
        "        ty = line[s:]\n",
        "        cwe = find_cwe(i,j)\n",
        "        if x:\n",
        "          if vul['CVSS']['version'] == '':\n",
        "            vul['CVSS']['version'] = cvss\n",
        "          vul['text'] = complete_vul_text\n",
        "          Vuls.append(vul)\n",
        "        x = 1\n",
        "      elif len(line.split())<=4:\n",
        "        try:\n",
        "          complete_vul_text = vul_text\n",
        "        except:\n",
        "          pass\n",
        "        vul_text = line\n",
        "        line_flag = 1\n",
        "        st = re.search('[A-Za-z]',line)\n",
        "        s = 0\n",
        "        if st is not None:\n",
        "          s = st.start()\n",
        "        ty = line[s:]\n",
        "        cwe = find_cwe(i,j)\n",
        "        if x:\n",
        "          if vul['CVSS']['version'] == '':\n",
        "            vul['CVSS']['version'] = cvss\n",
        "          vul['text'] = complete_vul_text\n",
        "          Vuls.append(vul)\n",
        "        x = 1\n",
        "      elif begin:\n",
        "        vul_text = vul_text +', ' + j\n",
        "        vul = process_vul(j,vul)\n",
        "    # start searching when starting divider occurs\n",
        "    if text_equal(j.upper(),vul_sec):\n",
        "      act = 1\n",
        "      flag = 1  \n",
        "      y = 1\n",
        "  # push side cases to other functions\n",
        "  if not x and y:\n",
        "    vul = side_case_vul(i)\n",
        "    return vul\n",
        "  elif not x and not y:\n",
        "    vul = []\n",
        "    return vul\n",
        "  else:\n",
        "    return Vuls"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcIfmk7yrMAN",
        "colab_type": "text"
      },
      "source": [
        "## 1.5 Get General Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLu0bT4TrLRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to get information from the text file\n",
        "def get_info(i):\n",
        "\n",
        "  Info = {}\n",
        "  # get the title and the link\n",
        "  Info['Title'] = i.split('.')[0]\n",
        "  Info['Link'] = \"https://www.us-cert.gov/ics/advisories/\"+Info['Title']\n",
        "  # get researcher \n",
        "  res_flag=0\n",
        "  Info['Researcher']=''\n",
        "  # get vulnerability information\n",
        "  Info['Vulnerabilities'] = vulner(i)\n",
        "  # get the vendor for the advisory\n",
        "  Info['Vendor'] = get_vendor(i)\n",
        "\n",
        "  for line in read_file(i):\n",
        "    if res_flag:\n",
        "      res_flag = 0\n",
        "      Info['Researcher'] = line\n",
        "    if re.search('researcher',line.lower()) is not None:\n",
        "      s = re.search('researcher',line.lower()).start()\n",
        "      if line[s:].lower() == 'researcher':\n",
        "        res_flag = 1\n",
        "\n",
        "  return Info"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJN-obDeSKBH",
        "colab_type": "text"
      },
      "source": [
        "# 2 Affected Products\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvsEs5kQNtSx",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 NLP processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mki0c_wo49Bh",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.1 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRka7f_p5Axx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilities for NLP processing for illegal structures\n",
        "# for strings in pname\n",
        "alert_st = ['VERB','AUX','X','PROPN PROPN PROPN NOUN NOUN','DET ADJ NOUN PUNCT'] \n",
        "unwanted_st = ['VERB AUX VERB','AUX ADV VERB','AUX PART VERB','VERB SCONJ PART',\n",
        "               'INTJ VERB','AUX VERB ADJ','VERB SCONJ DET','NOUN PUNCT PART',\n",
        "               'PUNCT NOUN PUNCT X PUNCT','AUX VERB AD','AUX ADV','VERB DET NOUN',\n",
        "               'PROPN AUX','AUX ADJ ADP','AUX SCONJ','PART VERB','ADV VERB ADP',\n",
        "               'DET NOUN VERB ADP','AUX PART ADJ','VERB ADP DET PROPN PROPN',\n",
        "               'AUX DET ADV','DET ADJ NOUN CCONJ','VERB PROPN PART','AUX NOUN ADP',\n",
        "               'VERB VERB ADP DET','DET ADJ NOUN PUNCT','ADJ NOUN VERB VERB',\n",
        "               'PROPN CCONJ PROPN VERB AUX NOUN']\n",
        "unwanted_p = ['.com','.htm','Note']\n",
        "# for strings in version\n",
        "unwanted_v = ['versions including:','not impacted','which is','versions including']\n",
        "unwanted_st_v = ['VERB ADV AUX VERB','DET AUX VERB PUNCT','SCONJ DET PROPN',\n",
        "                 'AUX VERB PART','AUX NOUN PUNCT','^NOUN VERB PUNCT$','^NOUN AUX VERB$',\n",
        "                 'VERB DET NOUN ADP','AUX AUX VERB SCONJ','NOUN AUX NOUN',\n",
        "                 'NOUN VERB ADV AUX VERB','DET NOUN AUX','NOUN VERB ADV AUX ADJ',\n",
        "                 'DET NOUN VERB AUX','NOUN VERB ADV PUNCT']\n",
        "unwanted_p_v = ['Only','These pumps were','This does not apply','In ']\n",
        "alert_st_v = ['AUX','X','VERB']"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKcSuXUQ5X8H",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.2 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD4JVfDyZOro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to generate a string of the sentence's POS tagging\n",
        "def sent_struct(line):\n",
        "  struct = ''\n",
        "  txt = nlp(line)\n",
        "  for t in txt:\n",
        "    struct = struct + ' ' + t.pos_\n",
        "  return struct"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeP_ELtGSoqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to filter affect product information with certain features\n",
        "def ap_filt(p):\n",
        "  # filter side cases\n",
        "  pname = p['Pname']\n",
        "  if '(enabled with iBed Wireless)' in pname:\n",
        "    return 1\n",
        "  if 'respective Ethernet communication modules listed below:' in p['version']:\n",
        "    return 1\n",
        "  # clean the product name\n",
        "  pname = pname.lstrip('·').rstrip(': ').rstrip(', ')\n",
        "  pname = re.sub(' und ', ' and ', pname)\n",
        "  pname = re.sub(' \\(.+\\):? ?$','',pname)\n",
        "  pname = pname.rstrip(', and')\n",
        "  pname = pname.rstrip(', with')\n",
        "  pname = pname.rstrip('with')\n",
        "  v = p['version']\n",
        "\n",
        "  # get the structure of product name and version\n",
        "  sent_ori_stct = sent_struct(pname)\n",
        "  sent_stct_v = sent_struct(v)\n",
        "  # examine whether the string may be useless according to structure\n",
        "  st_flag = find_sub(sent_ori_stct,alert_st)\n",
        "  st_flag_v = find_sub(sent_stct_v,alert_st_v)\n",
        "  # examine whether the string is useless according to structure\n",
        "  st_flag_2 = find_sub(sent_ori_stct,unwanted_st)\n",
        "  st_flag_2_v = find_sub(sent_stct_v,unwanted_st_v)\n",
        "  # examine whether the string is useless according to keywords\n",
        "  p_flag = find_sub(pname,unwanted_p)\n",
        "  p_flag_v = find_sub(pname,unwanted_p_v)\n",
        "  v_flag = find_sub(v,unwanted_v)\n",
        "  # calculate the final flag value\n",
        "  flag_p = st_flag*(st_flag_2+p_flag+v_flag)\n",
        "  flag_v = st_flag_v*(st_flag_2_v+p_flag_v)\n",
        "  flag = flag_p+flag_v\n",
        "  # add a side case\n",
        "  if flag or pname=='Note - Experion PKS':\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU2dH91zNxhW",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Structured Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCVBzufk27u7",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2.1 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm_DZUdD2-Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilities for get_div\n",
        "all_override = ['all versions prior','versions prior',': application versions',\n",
        "                ': All versions', ': all versions',', all versions',\n",
        "                ': All version',')  firmware version',') all versions',\n",
        "                ' all versions',': All firmware',': application versions',\n",
        "                ': Firmware V',': All Versions','All models','– All Versions',\n",
        "                ' (Firmware Version','TCSEFEC','^QJ71E71','All Models',\n",
        "                'running firmware versions prior to','^MV420','^MV440',\n",
        "                '^750-[0-9]{3,} firmware versions','(All Revisions)',\n",
        "                '^CPE[0-9]{3}','^CRU[0-9]{3}','^CPL[0-9]{3}',\n",
        "                '(End of Life; Upgrade to CPE330)','3\\.1 through 3\\.3\\.3']\n",
        "div_firmware = ['part n° 5.02','firmware version ']\n",
        "div_special = ['Dev.Rev.','Rev.','Rev ','Dev.Rev ']\n",
        "div_me = [': ME prior to V']\n",
        "div_kwds = ['(firmware|software) version','version','prior to ','revisions? ',\n",
        "          'all (firmware |software )?versions?',' series .+[0-9]+',\n",
        "      '( ver( |\\. )?|a|v( |\\.)?| \\(?r)?[0-9]+\\.([0-9]+\\.?)+',  \n",
        "      'back to', ' editions','firmware prior ','releases? ', 'running',\n",
        "      '( ver( |\\.)?|v( |\\.)?| \\(?r)[0-9]+', ' 20[0-1][0-9]( |,|/.)',\n",
        "      'serial numbers?','operating on ','all devices? ',' produced between ',\n",
        "      'wlan version ','manufactured before ','\\(shipped between',\n",
        "      ' up to and including ',' all windows',' older than',' \\(all model',\n",
        "      ' model ',' all current and older','mp[0-9]\\.[0-9]','\\(build','^XL1000C',\n",
        "      '^CPR9','^NC85','^NET Firmware','^ION','^PM5XXX','^OnCellG','^AWK-',\n",
        "      '^WAC-','^TAP-','^All ILC','^XLWeb','^1763-L','^750-820',' [0-9]\\.x ',\n",
        "      '^762-300','^1783-SAD','^2017 Update 2 and prior',', Models?',\n",
        "      ': article number','^HEN[0-9]{5,6}L?','^Security Notification SN',\n",
        "      '^H(B|E|4|2|P)(D|W)(3|8|2|4)P(R|C|E|1).{0,3}','^v12\\.001 and prior',\n",
        "      '^CP(E|U|L)(1|3|4)[0-3](0|2|5)','^Series PFC(1|2)00',\n",
        "      '^750-8[0-9][2-3], 750-83[1-2]','^v21\\.001 and prior',\n",
        "      'Product ID/Reorder number:','UDI/Model/NDC number:',', released',\n",
        "      'using Hi.+ version 0',', between','^1SAP1','^1TNE968','^TLXCD',\n",
        "      ', all current and older version']\n",
        "# Utilities for pull_head\n",
        "# for pname\n",
        "patterns_p = [' affects .+ listed', \n",
        "              'the sensys networks traffic sensors, .+ and .+,',\n",
        "              'the affected .+ version', ' (following|all) .+ appliances',\n",
        "              ' of .+?(,|are|is|:) ?', ' (following|all) .+ modules',\n",
        "              ' (following|all) .+ (versions?|products?|are|models?)']\n",
        "sub_p = [' of ',' are',' is',':',' following ', 'all ', '^the ','listed',\n",
        "         ' products and versions$','affected',' affects',\n",
        "         '(^hisecos)? (versions?|products?|models?|modules?)']\n",
        "# for version         \n",
        "patterns_v = ['distributed prior to .+, are','using .+ and lower ', \n",
        "              'with firmware .+( is| are|$|:)',\n",
        "              'running firmware versions prior to.+ of', \n",
        "              ' all versions prior to .+ of','prior to version .+?of ',\n",
        "              'version .+ and earlier are affected',\n",
        "              'to the following products using software.+',\n",
        "              ' of .+?(,|are|is|:) ?',' all versions? ', \n",
        "              'firmware version [A-Za-z0-9]+ (for|of|,|is|are|and)',\n",
        "              '(version)? [0-9\\.]+ and prior for the following']\n",
        "sub_v = [' for the following', ' for ',' of ',':',' is',' are',' affected',\n",
        "         'products using software',' of$','distributed',', regardless','with']"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nbxsAzg42PQ",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2.2 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE0-TyiqKaBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to pinpoint the product name and version in sentences\n",
        "def get_div(line, keys = []):\n",
        "  if 'TIA Portal V13 (including WinCC Professional Runtime):' in line:\n",
        "    return len(line)\n",
        "  elif re.search(' for:$',line) is not None:\n",
        "    return 0\n",
        "  line = line.rstrip('.')\n",
        "  vers_flag = re.search(' versions [0-9]\\.[0-9]{2,}', line)\n",
        "  # different occasions relate to different keywords package\n",
        "  if keys != []:\n",
        "    kwds = keys \n",
        "  elif find_sub(line,div_firmware,1):\n",
        "    kwds = ['firmware version ']\n",
        "  elif re.search('^(V|v)[0-9]+(\\.[0-9])?',line) is not None:\n",
        "    kwds = ['^(V|v)[0-9]+\\.[0-9]']           \n",
        "  elif find_sub(line, div_special):\n",
        "    kwds = div_special\n",
        "  elif re.search('[0-9]\\.[0-9]u[0-9]',line) is not None:\n",
        "    kwds = ['version','all versions']\n",
        "  elif find_sub(line, all_override) and vers_flag is None:\n",
        "    kwds = all_override\n",
        "  elif find_sub(line, div_me):\n",
        "    kwds = [' prior to']\n",
        "  else:\n",
        "    kwds = div_kwds\n",
        "  div = len(line)\n",
        "  # use the smallest index as the divider\n",
        "  for i in kwds:\n",
        "    if kwds == all_override:\n",
        "      i = re.sub('\\(','\\(',i)\n",
        "      i = re.sub('\\)','\\)',i)\n",
        "    if re.search(i.lower(), line.lower()) is not None:\n",
        "      if (i == ' series .+[0-9]+' or i == ' model ') and div != len(line):\n",
        "        break\n",
        "      new_div = re.search(i.lower(), line.lower()).start()\n",
        "      if new_div < div:\n",
        "        div = new_div\n",
        "  return div"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u55CdPnauRzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to extract information from lines in the head of a section\n",
        "def pull_head(line, type_is):\n",
        "  forall=''\n",
        "  # to extract product name\n",
        "  if type_is == 1:\n",
        "    pattern = patterns_p\n",
        "    subs = sub_p\n",
        "  # to extract version\n",
        "  elif type_is == 0:\n",
        "    pattern = patterns_v\n",
        "    subs = sub_v\n",
        "  # find certain patterns in a string\n",
        "  for i in pattern:          \n",
        "    if re.search(i, line.lower()) is not None:\n",
        "      forall = re.search(i, line.lower()).group()\n",
        "      if re.search(i, line) is not None:\n",
        "        forall = re.search(i, line).group().lower()\n",
        "      break\n",
        "  # clear noise in the string\n",
        "  for s in subs:\n",
        "    forall = re.sub(s,'',forall)\n",
        "  if not type_is:\n",
        "    forall = forall[get_div(forall):]\n",
        "  forall = forall.strip()\n",
        "  forall = re.sub('\\(','\\(',forall)\n",
        "  forall = re.sub('\\)','\\)',forall)\n",
        "  if re.search(forall,line.lower()) is None:\n",
        "    return 0\n",
        "  s = re.search(forall,line.lower()).start()\n",
        "  e = re.search(forall,line.lower()).end()\n",
        "  result = line[s:e]\n",
        "  return result"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ2jy_nCAyer",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Special Cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWuyVCWm2HmV",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3.1 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns2V16Ih2NIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilities for one_line\n",
        "kwrds_ol_verb = [' affects? ',' impacts? ',' reports? that ']\n",
        "to_sub_ol_kwds = [' are ',' software products ']\n",
        "# utilities for exceptions\n",
        "kwds_div = ['less than','equal to',', all versions.','prior to',\n",
        "                'using NTP service','using ',': All versions']\n",
        "kwds_find = ['less than','equal to',', all versions.',\n",
        "             'using NTP service','All products using ',\n",
        "             '0: All versions','Any DTM written']"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsgUG5082yfF",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3.2 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M98fiAgdrXko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to process affected products section with only one line\n",
        "def one_line(line):\n",
        "  # find the smallest index to divide\n",
        "  if find_key(line.lower(),kwrds_ol_verb) != len(line):\n",
        "    div = 0\n",
        "    for k_verb in kwrds_ol_verb:\n",
        "      new_div = re.search(k_verb, line.lower())\n",
        "      if new_div is not None:\n",
        "        new_div = new_div.end()\n",
        "        if new_div > div:\n",
        "          div = new_div\n",
        "    line = line[div:]\n",
        "  # extract information from generalizing sentence\n",
        "  pin = re.search('all .+ of .+ (are )?(.+)?',line.lower())\n",
        "  if pin is not None:\n",
        "    s = pin.start()\n",
        "    e = pin.end()\n",
        "    line = line[s:e]\n",
        "  # remove noise from certain sentences\n",
        "  div = find_key(line,to_sub_ol_kwds)\n",
        "  line = line[:div]\n",
        "  line = re.sub('(a|A)ll (known )?versions? of (the)?','', line).strip()\n",
        "  # remove 'all' at the beginning of a string\n",
        "  if line[:4].lower() == 'all ':\n",
        "    line = line[4:]\n",
        "  return line"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSM_Q3gYernf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to process sentences using 'all of' structure\n",
        "def all_of(n, lines, aff_prod = []):\n",
        "  cut = lines[n].split(' are')[0]\n",
        "  nxt_cut = ''\n",
        "  # examine whether the sentence should be processed\n",
        "  if n < len(lines)-1:\n",
        "    nxt_cut = lines[n+1].split(' are')[0]\n",
        "  aff_prot = {'Pname':cut,'version':'0'}\n",
        "  p_flag = ap_filt(aff_prot)\n",
        "  # stop processing if lines don't contain information\n",
        "  if not p_flag and not len(aff_prod):\n",
        "    return ['']\n",
        "  # pre-process lines with function one_line\n",
        "  p = one_line(lines[n])\n",
        "  # find seperate product names in following lines\n",
        "  if re.search(' versions? of ', nxt_cut) is not None:\n",
        "    temp = n\n",
        "    while re.search(' versions? of ', cut) is not None:\n",
        "      p = re.sub('of ','',re.search('of .+',cut).group())\n",
        "      v = re.search('back to.+',p)\n",
        "      if v is not None:\n",
        "        v = v.group()\n",
        "        p = re.sub(v,'',p)\n",
        "      else:\n",
        "        v = 'All versions'\n",
        "      temp += 1\n",
        "      cut = lines[temp].split(' are')[0]\n",
        "      aff_prod.append({'Pname':p,'version':v})\n",
        "    return aff_prod\n",
        "  # collect product information after related vulnerablities\n",
        "  if 'CVE' in cut:\n",
        "    p = lines[n].split(':')[1]\n",
        "    p = re.sub('of ','',re.search('of .+( |$)',p).group())\n",
        "    v = p[get_div(p):]\n",
        "    p = p[:get_div(p)]\n",
        "    aff_prod.append({'Pname':p,'version':v})\n",
        "    temp = n+1\n",
        "    nxt = lines[temp].split(':')[1]\n",
        "    while temp < len(lines):\n",
        "      nxts = nxt.split(' and ')\n",
        "      if not len(nxts)-1:\n",
        "        nxts = nxts[0].split(',')\n",
        "      for i in nxts:\n",
        "        aff_prod.append({'Pname':i,'version':''})\n",
        "      temp += 1\n",
        "      if temp < len(lines):\n",
        "        nxt = lines[temp].split(':')[1]\n",
        "    return aff_prod\n",
        "  # extract product information from the sentence\n",
        "  elif 'version' in p.lower():\n",
        "    to_sub = p\n",
        "    if re.search(' of.+',p) is not None:\n",
        "      p = re.sub(' of ','',re.search(' of.+',p).group())\n",
        "      v = re.sub(p, '', to_sub)\n",
        "    if 'are affected' not in lines[n] or 'are affected' not in p:\n",
        "      keys=['prior to','using']\n",
        "      v = lines[n][find_key(lines[n],keys):].split(' are')[0]\n",
        "      p = re.sub(v,'',p)\n",
        "    return {'Pname':p,'version':v}\n",
        "  else:\n",
        "    v = 'All Versions'\n",
        "  # extract seperate version in following lines\n",
        "  if ':' == p[-1]:\n",
        "    temp = n+1\n",
        "    nxt = lines[temp]\n",
        "    while 'versions of' not in nxt:\n",
        "      v = nxt\n",
        "      if '(' in nxt:\n",
        "        aff_prod[-1]['version'] = aff_prod[-1]['version'] + nxt\n",
        "        temp += 1\n",
        "        if temp >= len(lines):\n",
        "          break\n",
        "        nxt = lines[temp]\n",
        "        continue\n",
        "      aff_prod.append({'Pname':p,'version':v})\n",
        "      temp += 1\n",
        "      if temp >= len(lines):\n",
        "        break\n",
        "      nxt = lines[temp]\n",
        "    if temp < len(lines):\n",
        "      all_of(temp, lines,aff_prod)\n",
        "    return aff_prod\n",
        "  return {'Pname':p,'version':v}"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpuy9wycm4wI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to extract information from the rest side cases\n",
        "def exceptions(n, lines):\n",
        "  line = lines[n]\n",
        "  # skip 'not affected' information\n",
        "  if 'not affected' in line.lower():\n",
        "    return 1\n",
        "  else:\n",
        "    line = lines[n]\n",
        "    # divide certain sentences to extract information\n",
        "    if find_sub(line,kwds_find):\n",
        "      d = get_div(line, kwds_div)\n",
        "      p = line[:d]\n",
        "      v = line[d:]\n",
        "      return (1,{'Pname':p,'version':v})\n",
        "    # extract information using function one_line\n",
        "    elif re.search('All [A-Za-z ]+ are .+ed', line) is not None:\n",
        "      p = one_line(line)\n",
        "      v = ''\n",
        "      return [1,{'Pname':p,'version':v}]\n",
        "    # process side cases with 'with' in it\n",
        "    elif 'with:·' in line:\n",
        "      output = []\n",
        "      line = line.split('with:·')[1]\n",
        "      while n < len(lines):\n",
        "        line = lines[n].split('with:·')[-1]\n",
        "        d = get_div(line)\n",
        "        p = line[:d]\n",
        "        v = line[d:]\n",
        "        output.append({'Pname':p,'version':v})\n",
        "        n += 1\n",
        "      return [2, output]\n",
        "    # process a specific side case\n",
        "    elif re.search('^DirectLogic DL', line) is not None:\n",
        "      p = line\n",
        "      n += 1\n",
        "      output = []\n",
        "      v = lines[n]\n",
        "      while re.search('^DirectLogic DL', v) is None:\n",
        "        output.append({'Pname':p,'version':v})\n",
        "        n += 1\n",
        "        if n == len(lines):\n",
        "          break\n",
        "        v = lines[n]\n",
        "      return [3, output]\n",
        "    else:\n",
        "      return 1"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx-FCsAYOvni",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Extract Product Info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbNrvSzPt_N6",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4.1 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69d0XZLxuLyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilities for getting affected products\n",
        "special = ['Dev.Rev.','Cond.','Rev.','Ing', 'ver.',\n",
        "           'Rev','Dev.Rev','Cond','ver']\n",
        "fisrt_sign = ['following','affected products','products affected:','products:',\n",
        "              ' for profinet io:', 'reports that the vulnerability affects',\n",
        "              'the affected codesys runtime version is part of']\n",
        "stct_unexpected = ['VERB','AUX']\n",
        "void_line = ['.','']\n",
        "G_cases = ['G-Cam:','G-Code:']"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIcXeQNw1cNk",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4.2 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnBrI8Vn4BUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to extract affected products' information\n",
        "def get_ap(i):\n",
        "\n",
        "  v_occupied = 0\n",
        "  \n",
        "  ap = []\n",
        "  pot = ''\n",
        "  list_adv = collect_lines(i)[1]\n",
        "  if len(list_adv) == 0:\n",
        "    return []\n",
        "  num_lines = len(list_adv)\n",
        "  j = 0\n",
        "\n",
        "  # process side cases with version information in the end\n",
        "  if 'For the listed products' in list_adv[-1]:\n",
        "    vers = list_adv[-1]\n",
        "    vend = list_adv[0]\n",
        "    v = re.search('all.+[0-9]',vers).group()\n",
        "    ven = re.search('following.+models',vend).group().split()[1]\n",
        "    for line in list_adv[1:-1]:\n",
        "      for p in line.split(', '):\n",
        "        ap.append({'Pname':ven+' '+p.lstrip('·'),'version':v})\n",
        "    return ap\n",
        "  # process cases with only one line\n",
        "  if num_lines == 1:\n",
        "    l = list_adv[0]\n",
        "    if re.search(' versions? of ', l.lower()) is not None and 'InTouch' not in l:\n",
        "      result = all_of(j, list_adv, [0])\n",
        "      return [result]\n",
        "    d_ori = get_div(l)\n",
        "    p = l[:d_ori]\n",
        "    v = l[d_ori:]\n",
        "    if v in void_line:\n",
        "      p = pull_head(l, 1)\n",
        "      v = ''\n",
        "      if not p:\n",
        "        return []\n",
        "    if find_sub(sent_struct(p),stct_unexpected):\n",
        "      ol = one_line(l)\n",
        "      d = get_div(ol)\n",
        "      p = ol[:d]\n",
        "      v = ol[d:]\n",
        "      if v in void_line:\n",
        "        v = 'All Versions'\n",
        "    ap.append({'Pname':p,'version':v})\n",
        "    return ap\n",
        "\n",
        "  first_line = 0\n",
        "  while j < num_lines:\n",
        "    \n",
        "    # remove noise strings in the line\n",
        "    ori_l = list_adv[j].rstrip('.').lstrip('·')\n",
        "    if '•\\xa0\\xa0 \\xa0' in ori_l:\n",
        "      ori_l = ori_l[5:]\n",
        "    elif '\\xa0 \\xa0 \\xa0 \\xa0\\xa0 ' in ori_l:\n",
        "      ori_l = ori_l[9:]\n",
        "    elif '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0' in ori_l:\n",
        "      ori_l = ori_l[9:]\n",
        "    l = ori_l.lower()\n",
        "    # process side cases with useless strings\n",
        "    if pot == 'special-1':\n",
        "      p = re.sub('(All | are affected )','',ori_l)\n",
        "      v = ''\n",
        "      ap.append({'Pname':p,'version':v})\n",
        "      return ap\n",
        "    # skip side cases with certain information\n",
        "    if 'For the Alaris Gateway Workstation' in ori_l:\n",
        "      j+=1\n",
        "      continue\n",
        "    # strip phrases in a sentence\n",
        "    if ori_l.split()[-1] == 'Versions' and ', All Versions' not in ori_l:\n",
        "      ori_l = re.sub('Versions','',ori_l)\n",
        "      l = ori_l.lower()\n",
        "    # skip lines as subsection divider\n",
        "    if re.search('4\\.1\\.[0-9]\\xa0',ori_l) is not None:\n",
        "      j+=1\n",
        "      pot = ''\n",
        "      first_line = 1\n",
        "      continue\n",
        "    # skip lines that don't initiate rightly\n",
        "    if not first_line and not find_sub(l,fisrt_sign):\n",
        "      if 'This vulnerability impacts' in ori_l:\n",
        "        ori_l = ori_l.split('impacts ')[1]\n",
        "        l = ori_l.lower()\n",
        "        first_line = 1\n",
        "        pot = 'special-1'\n",
        "        pass\n",
        "      # process exceptions\n",
        "      else:\n",
        "        flag_excpt = exceptions(j, list_adv) \n",
        "        if flag_excpt == 1:\n",
        "          j+=1\n",
        "          continue\n",
        "        elif flag_excpt[0] == 1:\n",
        "          ap.append(flag_excpt[1])\n",
        "          j+=1\n",
        "          continue\n",
        "        elif flag_excpt[0] > 2:\n",
        "          ap = ap + flag_excpt[1]\n",
        "          j += len(flag_excpt[1])+1\n",
        "          continue\n",
        "        else:\n",
        "          return flag_excpt[1]\n",
        "    # skip lines with 'not affected' information\n",
        "    if 'not affected' in l:\n",
        "      j+=1\n",
        "      first_line = 0\n",
        "      continue\n",
        "    if find_sub(l, fisrt_sign):\n",
        "      # skip when the report starts talking about vendors\n",
        "      if re.search('the following vendor companies:$', l) is not None:\n",
        "        j+=1\n",
        "        continue\n",
        "      # skip when MLFBs are to be referred\n",
        "      elif 'with the following MLFBs' in ori_l:\n",
        "        first_line = 0\n",
        "        j+=1\n",
        "        continue\n",
        "      pot = ori_l\n",
        "      first_line = 1\n",
        "    # process lines using all_of function\n",
        "    elif re.search(' versions? of ', l) is not None and 'InTouch' not in ori_l:\n",
        "      result = all_of(j, list_adv, [])\n",
        "      if type(result) == dict or result == ['']:\n",
        "        if result != ['']:\n",
        "          ap.append(result)\n",
        "        j += 1\n",
        "        continue\n",
        "      else:\n",
        "        return result\n",
        "\n",
        "    # process cases that are more general \n",
        "    else:\n",
        "      d_ori = get_div(ori_l)\n",
        "      p = ori_l[:d_ori]\n",
        "      v = ori_l[d_ori:]\n",
        "      # process side cases with 'G-Cam:' and 'G-Code:'\n",
        "      if find_sub(p, G_cases): \n",
        "        p = ''\n",
        "        d_ori = 0\n",
        "      # complete lines that are mistakenly split\n",
        "      if ori_l.split()[-1] in special:\n",
        "        j+=1\n",
        "        if ori_l[-1] != '.':\n",
        "          ori_l = ori_l + '.'\n",
        "        ori_l = ori_l + list_adv[j]\n",
        "        d = get_div(ori_l)\n",
        "        p = ori_l[:d]\n",
        "        v = ori_l[d:]\n",
        "      # extract continuously in cases with version/pname only appear in the first line\n",
        "      elif (d_ori == len(l) or d_ori == 0) and j < len(list_adv)-1:\n",
        "        d = d_ori\n",
        "        temp = j+1\n",
        "        now = j\n",
        "        next_line = list_adv[temp].rstrip('.').lstrip('·').strip('•\\xa0\\xa0 \\xa0')\n",
        "        mark = len(next_line)-len(next_line)*(d>1)\n",
        "        # stops when the line is not pure pname/version\n",
        "        while get_div(next_line) == mark:\n",
        "          if d == 0:\n",
        "            p = next_line\n",
        "          elif d == len(l):\n",
        "            v = next_line\n",
        "          if re.search(' for:$',l) is not None:\n",
        "            v = ori_l.split(' of')[0]\n",
        "          if p == 'This notification can be found at the following location:':\n",
        "            next_line = ''\n",
        "            continue\n",
        "          # process side cases with CVE information\n",
        "          if 'CVE-2019-1093' in p:\n",
        "            if 'SIPROTEC 5 with CPU variants CP200' in ap[-1]['Pname']:\n",
        "              p = ap[-1]['Pname']\n",
        "              if ap[-1]['version'] == '':\n",
        "                ap.pop()\n",
        "          aff_prod = {'Pname':p,'version':v}\n",
        "          aff_flag = ap_filt(aff_prod)         \n",
        "          if aff_flag:\n",
        "            ap.append({'Pname':p,'version':v})\n",
        "          temp+=1\n",
        "          if temp >= len(list_adv):\n",
        "            break\n",
        "          next_line = list_adv[temp].rstrip('.').lstrip('·').strip('•\\xa0\\xa0 \\xa0')\n",
        "          mark = len(next_line)-len(next_line)*(d>1)\n",
        "        j = temp-1\n",
        "        # look for information in the first lines\n",
        "        if now == j:\n",
        "          # look for product name\n",
        "          if d_ori == 0:\n",
        "            p = pull_head(pot, 1)\n",
        "          # look for version\n",
        "          elif d_ori == len(l):\n",
        "            v = pull_head(pot, 0)\n",
        "          if p != '' and v != '':\n",
        "            # process side cases with certain version format\n",
        "            sc_sap = re.search('^1SAP1',v)\n",
        "            sc_tne = re.search('^1TNE968',v)\n",
        "            if sc_sap is not None or sc_tne is not None:\n",
        "              p = pull_head(pot,1) + ' ' + v\n",
        "              v = pull_head(pot,0)\n",
        "            ap.append({'Pname':p,'version':v})\n",
        "      # process lines at the end of the document with partial information\n",
        "      elif (d_ori == len(l) or d_ori == 0) and j == len(list_adv)-1:\n",
        "        if d_ori == 0:\n",
        "          p = pull_head(pot, 1) # pull head\n",
        "        elif d_ori == len(l):\n",
        "          v = pull_head(pot, 0)\n",
        "      # skip 'not affected' lines\n",
        "      else:\n",
        "        if 'not affected' in l:\n",
        "          j+=1\n",
        "          continue\n",
        "      # reprocess lines with wrong division\n",
        "      if v[:4] == 'V11 ' and 'all version' in v.lower():\n",
        "        p = p + 'V11'\n",
        "        v = 'All versions'\n",
        "      elif v[:3] == 'V3 ' or v == 'V3' or v == 'V3,' or v[:4] == 'V11 ':\n",
        "        p = p + v\n",
        "        v = pull_head(pot, 0)\n",
        "      elif re.search('^(V|v)[0-9]',p) is not None and 'all version' in p.lower():\n",
        "        v = p\n",
        "        p = ap[-1]['Pname']\n",
        "        if ap[-1]['version'] == '':\n",
        "          ap.pop()\n",
        "      # supplement lines missing information\n",
        "      if 'WEB’log' in pot:\n",
        "        p = 'WEB’log ' + p\n",
        "      aff_prod = {'Pname':p,'version':v}\n",
        "      # filter lines with certain POS taggings or text\n",
        "      # st = sent_struct(p)\n",
        "      p_flag = ap_filt(aff_prod)\n",
        "      if p == '\\t':\n",
        "        ap[-1]['version'] = v\n",
        "        p_flag = 0\n",
        "      if aff_prod not in ap and p_flag:\n",
        "        ap.append(aff_prod)\n",
        "    j+=1\n",
        "  return ap"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJhForRP4lgC",
        "colab_type": "text"
      },
      "source": [
        "### 2.5 Version conversion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "porYeHt-UypW",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.1 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc5iHyd4GiLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilities for version conversion\n",
        "words_of_interest = ['prior', 'earlier', 'through', 'before','up to','between',\n",
        "                     'after', 'back to','older than','prior to','including',\n",
        "                     'excluding', 'from','or lower','or older','or higher',\n",
        "                     'and previous versions','and later','and newer',\n",
        "                     'and lower','and below','and greater','and above',\n",
        "                     'and potentially prior','and previous','and older',\n",
        "                     'all versions earlier than','and all previous versions',\n",
        "                     'or earlier','all subsequent minor releases','and prior',\n",
        "                     'and possibly earlier versions','and any previous version']\n",
        "\n",
        "sub_in_v_c = [' ?all versions earlier than ?',\n",
        "              ' ?and all subsequent minor releases ?',\n",
        "              ' ?and possibly earlier versions ?',' ?and any previous version ?',\n",
        "              ' ?and previous versions ?', ' ?and all previous versions ?,?', \n",
        "              'devices that include the ', ' ?and possibly earlier versions ?', \n",
        "              ' ?back to ?', ' ?or earlier ?', ' ?(A|a)ll (?!firmware )versions? ?,?',\n",
        "              '^(v|V)ersions(:·| )?', ' ?(A|a)ll ', ' ?shipped between ?', \n",
        "              ' (excluding|including) ?', ' ?up to ?', ' ?(or )?prior ?',\n",
        "              ' ?older than ?', ' ?and potentially prior releases ?',\n",
        "              ' ?and (all )?prior ?',' ?newer ?',' ?(or )?lower ?',' ?below ?',\n",
        "              ' ?later ?',' ?(or )?higher ?',' ?above ?',' ?previous ',\n",
        "              ' ?greater ?', ' ?(or )?older ?',' ?than ?',' and ?',\n",
        "              ' ?prior ?(to )?',' ?(or )?earlier ?(than )?', ' ?before ?',\n",
        "              ' ?(v|V)ersions(,|:·|;| |, and| )?$',' ?possibly ?',' of the ']\n",
        "\n",
        "# keywords related to smaller/smaller or equal to\n",
        "prece = ['and earlier','prior to','before','up to','and prior','and below',\n",
        "          'and all prior','or lower','and previous versions','and lower','prior',\n",
        "          'and potentially prior','and previous','all versions earlier than',\n",
        "          'and all previous versions','or earlier','and possibly earlier versions',\n",
        "          'lower than','and any previous version']\n",
        "prece_incl = ['and earlier','up to','and prior','and below','and potentially prior',\n",
        "              'and all prior','or lower','and previous versions','and lower',\n",
        "              'and previous','and all previous versions','or earlier',\n",
        "              'and possibly earlier versions','including','and any previous version']\n",
        "# keywords related to larger/larger or equal to\n",
        "subse = ['and all subsequent minor releases','older than','or higher','and later',\n",
        "          'and newer','back to','and greater','and above','and older']\n",
        "subse_incl = ['and all subsequent minor releases','or higher','and above','and later',\n",
        "              'and newer','and greater','and older','including','back to']"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVWRGNyUsJEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unique utilities for cut_useless\n",
        "skip_cut_kwds = ['between','MLFB 6SR']\n",
        "add_to_kwds = ['including','Patch']\n",
        "# unique utilities for get_ver\n",
        "move_all_kwds = ['between','starting at ','through']\n",
        "two_side_kwds = ['including','excluding']\n",
        "# unique utilities for ver_con\n",
        "extract_st = ['ADV VERB DET','ADV VERB PRONP']\n",
        "kwrds_unwanted = ['.xlsx','not affected','.com','CVE',\n",
        "                  'News updates','Wind River System products']\n",
        "void_head = ['Quantum','Premium','M340']\n",
        "prod_conj = [' and ', ' of ']\n",
        "key_verb = ['impacts','affects']"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIldgzfVtoAR",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.2 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMIEgCSQz82P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to remove useless information from version\n",
        "def cut_useless(ver):\n",
        "  if re.search('\\(.+\\),?$',ver) is not None:\n",
        "    out = ver\n",
        "    return out\n",
        "  elif re.search('^750-8[0-9][2-3], 750-83[1-2]',ver) is not None:\n",
        "    out = ver\n",
        "    return out\n",
        "  elif find_sub(ver,skip_cut_kwds):\n",
        "    out = ver\n",
        "    return out\n",
        "  out = ''\n",
        "  for i in ver.split(','):\n",
        "    if i == ' all versions affected':\n",
        "      continue\n",
        "    i = i + ' '\n",
        "    if find_sub(i,words_of_interest):\n",
        "      to_sub = i\n",
        "      for s in sub_in_v_c:\n",
        "        to_sub = re.sub(s,'',to_sub)\n",
        "      if re.sub(' ','',to_sub) == '':\n",
        "        out = i + ' ' + out[1:]\n",
        "      elif 'and all previous versions' in i:\n",
        "        i = re.sub('and all previous versions','',i)\n",
        "        out = 'and all previous versions' + ' ' + out + ', ' + i \n",
        "      else:\n",
        "        if find_sub(i,add_to_kwds):\n",
        "          out = out + ', ' + i\n",
        "        else:\n",
        "          out = out + ' ' + i\n",
        "    elif get_div(i) <= 1 or 'and' in i[:4]: \n",
        "      out = out+', '+i\n",
        "    elif 'or' in i[:3] or '&' in i:\n",
        "      out = out+', '+i\n",
        "    elif re.search('^( [0-9]+| released)',i) is not None:\n",
        "      out = out+', '+i \n",
        "    elif re.search('^( ?G[0-9][0-9]| Service Pack| Patch) ?',i) is not None:\n",
        "      out = out+', '+i \n",
        "  return out.lstrip(', ')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc07PQaljkIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to locate where the inequality is\n",
        "def pinpoint_sign(vers, rule, mark):\n",
        "  parts = []\n",
        "  slices = vers.split(', ')\n",
        "  for i in slices:\n",
        "    part = i\n",
        "    if find_sub(re.sub('\\(.+\\)','',i), rule):\n",
        "      if i == 'and any previous version':\n",
        "        part = ', '.join(slices[:-1])\n",
        "        parts = []\n",
        "      part = mark + re.sub('^, ','',part)\n",
        "    parts.append(part)\n",
        "\n",
        "  return ', '.join(parts)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhhPuf2C1kkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to get the value of versions\n",
        "def get_ver(vers):\n",
        "  # Strip off irrelevant words\n",
        "  vers = re.sub(',  including',', containing', vers)\n",
        "  if find_sub(vers,['<','>','=']):\n",
        "    vers = re.sub('^((A|a)ll )?(V|v)ersions ?','',vers)\n",
        "  if find_sub(vers.lower(),words_of_interest):\n",
        "    # Strip off 'are affected' part at the end of a string\n",
        "    if ' are ' in vers:\n",
        "      vers = vers.split(' are ')[0]\n",
        "    if 'all versions through' in vers.lower():\n",
        "      value = '<= ' + re.sub('all versions through','',vers.lower())\n",
        "    elif vers == 'all current and older versions':\n",
        "      value = vers\n",
        "    elif re.search('after .+ and before', vers) is not None:\n",
        "      value = vers[re.search('after .+ and before', vers).start():]\n",
        "    elif find_sub(vers.lower(),prece) and find_sub(vers.lower(),subse):\n",
        "      value = re.sub('((A|a)ll )?(v|V)ersions','',vers)\n",
        "    elif find_sub(vers.lower(),move_all_kwds):\n",
        "      value = re.sub('((A|a)ll )?(V|v)ersions ?','',vers)\n",
        "    elif find_sub(vers.lower(),two_side_kwds,1):\n",
        "      value = vers\n",
        "    # Transform version string to general expressions with inequality\n",
        "    elif find_sub(vers.lower(),prece):\n",
        "      mark = '< '\n",
        "      if find_sub(vers,prece_incl) and re.search('\\(including ', vers) is None:\n",
        "        mark = '<= '\n",
        "      if ', ' in vers:\n",
        "        vers = pinpoint_sign(vers, prece, mark)\n",
        "      else:\n",
        "        vers = mark + vers\n",
        "      for i in sub_in_v_c:\n",
        "        vers = re.sub(i,' ',vers)\n",
        "      value = re.sub('^<= (v|V)ersions(:·| )?','<= ',vers)\n",
        "      value = re.sub('^< (v|V)ersions(:·| )?','< ',value)\n",
        "    elif find_sub(vers.lower(),subse):\n",
        "      mark = '> '\n",
        "      if find_sub(vers,subse_incl) and re.search('\\(including ', vers) is None:\n",
        "        mark = '>= '\n",
        "      if ', ' in vers:\n",
        "        vers = pinpoint_sign(vers, subse, mark)\n",
        "      else:\n",
        "        vers = mark + vers\n",
        "      for i in sub_in_v_c:\n",
        "        vers = re.sub(i,' ',vers)\n",
        "      value = re.sub('^>= (v|V)ersions(:·| )?','>= ',vers)\n",
        "      value = re.sub('^> (v|V)ersions(:·| )?','> ',value)\n",
        "    else:\n",
        "      value = vers\n",
        "  elif vers == 'Versions:·3.5':\n",
        "    value = re.sub('Versions:·','',vers)\n",
        "  else:\n",
        "    value = vers\n",
        "  if '(' not in value and value[0]!='(':\n",
        "    value = value.rstrip(')')\n",
        "  # Clean the remaining noise from value string\n",
        "  value=re.sub(' +',' ',value)\n",
        "  value=re.sub('\\.\\.\\.','to',value)\n",
        "  value=re.sub(' ,',',',value)\n",
        "  return value"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-ReSG974qY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to add version value to affected products' information\n",
        "def ver_con(i):\n",
        "  output = []\n",
        "  ap = get_ap(i)\n",
        "  \n",
        "  for p in ap:\n",
        "    store = []\n",
        "    line = p['Pname']+p['version']\n",
        "    \n",
        "    # skip certain product name which is actually sub-section divider\n",
        "    if p['Pname'] in void_head:\n",
        "      continue\n",
        "    # store another affected product in the line \n",
        "    if ': Catapult ' in p['version']:\n",
        "      store.append(p['version'].split(':')[1])\n",
        "      p['version'] = p['version'].split(':')[0]\n",
        "    # clean product names with certain errors\n",
        "    elif re.search('^(Ind|D)irectly affected:',p['Pname']):\n",
        "      p['Pname'] = p['Pname'].split(':')[1]\n",
        "    elif p['Pname'] == 'and':\n",
        "      p['Pname'] = ap[0]['Pname']\n",
        "    elif re.search('all .+ versions (are affected)?.?$',line.lower()) is not None:\n",
        "      p['Pname'] = re.sub('(A|a)ll|versions?|are affected','',line)\n",
        "      p['version'] = 'All versions'\n",
        "    elif find_sub(p['Pname'],kwrds_unwanted) or 'products:' == p['Pname'].split()[-1]:\n",
        "      continue\n",
        "    elif find_sub(p['version'],[': All versions',': (End of Life; Upgrade to CPE330)']):\n",
        "      p['Pname'] = p['Pname']+' '+p['version'].split(':')[0]\n",
        "      p['version'] = p['version'].split(':')[1]\n",
        "    elif 'Version' in p['Pname']:\n",
        "      d = get_div(p['Pname'],[' Version'])\n",
        "      p['version'] = p['Pname'][d:]\n",
        "      p['Pname'] = p['Pname'][:d]\n",
        "    p['Pname'] = re.sub('(\\(CPR9|versions)$','',p['Pname'])\n",
        "    # skip when pname is void or the sentence is a generalizing one\n",
        "    if p['Pname'] == '':\n",
        "      pass\n",
        "    elif p['Pname'][-1] == ':' and p['version']=='':\n",
        "      continue\n",
        "    elif re.search('^Ships with ', p['Pname']) is not None:\n",
        "      continue\n",
        "    # clean product name information with unnecessary words\n",
        "    Pname = p['Pname']\n",
        "    if Pname[:4].lower() == 'all ':\n",
        "      Pname = Pname[4:]\n",
        "    elif Pname[:4].lower() == 'any ':\n",
        "      Pname = Pname[4:]\n",
        "    while polish(Pname)!=Pname:\n",
        "      Pname = polish(Pname)\n",
        "    if 'listed in the mitigations' in Pname:\n",
        "      Pname = Pname.split(' prior to applying the patch')[0]\n",
        "    if ' Works together with other products listed here' in Pname:\n",
        "      Pname = Pname.split(':')[0]\n",
        "    p['Pname'] = Pname.split(', which ')[0]\n",
        "    # clean version information\n",
        "    version = cut_useless(p['version'])    \n",
        "    ori_ver = p['version']\n",
        "    if re.search('4\\.1\\.[0-9]\\xa0\\xa0 \\xa0',p['version']) is not None:\n",
        "      version = ''\n",
        "    if ') with CPU variants CP300 and CP100' in version:\n",
        "      version = version.split(')')[0]\n",
        "    version = version.split(' are affected')[0]\n",
        "    version = version.split(' is affected')[0]\n",
        "    version = version.split(' is impacted')[0]\n",
        "    version = version.split(' could be affected')[0]\n",
        "    version = re.sub('p;rior','prior',version)\n",
        "    version = re.sub('\\xa0',' ',version)\n",
        "    while polish(version)!=version:\n",
        "      version = polish(version)\n",
        "    if version == 'versions including':\n",
        "      continue\n",
        "    p['version'] = version\n",
        "    # get version value from function get_ver\n",
        "    if p['version'] != '' and p['version'].lower()!='all versions':\n",
        "      vers = get_ver(p['version'])\n",
        "      if '):' in version:\n",
        "        version = version.split('):')[0]\n",
        "      p['version'] = vers\n",
        "    # complete version value that are wrongly split\n",
        "    if p['version'] == 'versions':\n",
        "      p['version'] = 'All Versions'\n",
        "    # process another product information if there's any\n",
        "    if store != []:\n",
        "      cont = store[0]\n",
        "      d = get_div(cont)\n",
        "      pname = cont[:d]\n",
        "      ver = cont[d:]\n",
        "      vers = get_ver(ver)\n",
        "      ap.append({'Pname':pname, 'version':vers})\n",
        "    # process certain product information if pname is in version\n",
        "    if find_sub(p['version'],prod_conj,1):\n",
        "      p['Pname'] = vers.split(' of ')[1]\n",
        "      p['version'] = vers.split(' of ')[0]\n",
        "    # skip invalid information\n",
        "    if version == '2015/01/20-001 addresses':\n",
        "      continue\n",
        "    if p['Pname'] == 'product and versions':\n",
        "      p['Pname'] = output[0]['Pname']\n",
        "    # extract production information from pname with certain POS format\n",
        "    p_st = sent_struct(p['Pname'])\n",
        "    if find_sub(p_st,extract_st):\n",
        "      key = find_key(p['Pname'],key_verb)\n",
        "      p['Pname'] = re.sub('(impacts|affects) the','',p['Pname'][key:])\n",
        "    p_flag = re.search('^R4[0-9]',p['Pname']) is not None\n",
        "    v_flag = re.search('^R4[0-9]',p['version']) is not None\n",
        "    if p_flag*v_flag:\n",
        "      prod = re.sub('R|[0-9]\\.?|release|and','',p['version']).strip()\n",
        "      p['version'] = re.sub((prod+' '),'',p['Pname'] + ', '+ p['version'])\n",
        "      p['Pname'] = prod\n",
        "    # clean the version text\n",
        "    p['version'] = re.sub(',,','',p['version'])\n",
        "    cell = {'Pname':p['Pname'],'version':p['version'],\n",
        "            'version text':ori_ver}\n",
        "    if cell not in output:\n",
        "      output.append({'Pname':p['Pname'],'version':p['version'],\n",
        "                     'version text':ori_ver})\n",
        "  return output"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxqIoHsJxrFp",
        "colab_type": "text"
      },
      "source": [
        "# 3 Metrics Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHNOfDe9L4Qp",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Vulnerability Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULdH1RT26p6Y",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1.1 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrDubr6N6xDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilities for collecting key lines\n",
        "ui_special = ['user interaction','user ','social engineering']\n",
        "av_special = ['local access','physical access']\n",
        "av_special_more = ['remote','local','adjacent']\n",
        "ac_special = ['skill',' difficult ']\n",
        "av = ['physical access','remote','local','adjacent','network access']\n",
        "ac = ['skill',' difficult ','easy','crafting','simple']\n",
        "ui_1 = ['social engineering','triggered']\n",
        "ui_2 = [' or ',' user ']\n",
        "ui_3 = ['user interaction','user ']\n",
        "# utilities for extracting metrics value\n",
        "met_val = {'UI':{'can be exploited':'None'},\n",
        "           'AV':{'local':'Local','remote':'Network','adjacent':'Adjacent',\n",
        "                 'physical':'Physical','network':'Network'},\n",
        "           'AC':{' to medium':'',' to high':'',' to moderate':'','-medium':'',\n",
        "                 '-high':'','-moderate':'', 'moderate ':'case','medium':'case', \n",
        "                 'difficult':'High', 'low':'Low','High':'High',\n",
        "                 'moderately difficult':'High','easy':'Low', 'simple':'Low', \n",
        "                 'more skilled':'High', 'take some effort':'High'}}\n",
        "metric_sc = ['some','both','not ?.+(remotely| remotely)',\n",
        "             'except for','(one|three) of ']         \n",
        "typical = ['^(These|This|The) vulnerabilit(ies|y) ','^(The|An) attacker',\n",
        "           '^All the vulnerabilities','^Physical network','^This exploit',\n",
        "           '^In order to','^Exploitation of']                   "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po2MD3m_9JuK",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1.2 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz9FVo8IvEFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to check whether a sentence have more than one metrics\n",
        "def see_sides(line):\n",
        "  result = []\n",
        "  for i in line:\n",
        "    if find_sub(i.lower(), ui_special):\n",
        "      result.append(1)\n",
        "    elif find_sub(i.lower(), av_special):\n",
        "      result.append(3)\n",
        "    elif find_sub(i.lower(), av_special_more):\n",
        "      result.append(2)\n",
        "    elif find_sub(i.lower(), ac_special):\n",
        "      result.append(3)\n",
        "  r = len(set(result)) > 1\n",
        "  return r"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7fAs7w8dl23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to divide lines with metric information\n",
        "def split_lines(r):\n",
        "  output = []\n",
        "  if '; ' in r:\n",
        "    output = output + r.split('; ')\n",
        "  elif ' via ' in r.lower():\n",
        "    if see_sides(r.lower().split(' via ')):\n",
        "      output = output + r.lower().split(' via ')\n",
        "    else:\n",
        "      output.append(r)\n",
        "  elif ' but ' in r.lower():\n",
        "    if see_sides(r.lower().split(' but ')):\n",
        "      output = output + r.lower().split(' but ')\n",
        "    else:\n",
        "      output.append(r)\n",
        "  elif ' and ' in r.lower():\n",
        "    if see_sides(r.lower().split(' and ')):\n",
        "      output = output + r.lower().split(' and ')\n",
        "    else:\n",
        "      output.append(r)\n",
        "  else:\n",
        "    output.append(r)\n",
        "  return output"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lhrbPlYUAU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to indentify sentences that state the metrics from the file\n",
        "def properties(line, sort):\n",
        "  p = {}\n",
        "  # process text within sections\n",
        "  if sort:\n",
        "    for i in line:\n",
        "      j = split_lines(i)\n",
        "      for k in j:\n",
        "        if find_sub(k.lower(),ui_1):\n",
        "          if 'remotely' in k.lower() and 'AV' not in p.keys():\n",
        "            p['AV']=k.strip().title()\n",
        "          if 'UI' not in p.keys():\n",
        "            p['UI']=k.strip().title()\n",
        "          else:\n",
        "            continue\n",
        "        elif find_sub(k.lower(), ac) and 'AC' not in p.keys():\n",
        "          p['AC']=k.strip().title()\n",
        "        elif find_sub(k.lower(), av) and 'AV' not in p.keys():    \n",
        "          p['AV']=k.strip().title()\n",
        "          if find_sub(k.lower(), ui_2, 1):\n",
        "            p['UI']=k.strip().title()\n",
        "        elif find_sub(k.lower(), ui_3) and 'UI' not in p.keys():\n",
        "          p['UI']=k.strip().title()\n",
        "  # process metrics of one line format\n",
        "  else:\n",
        "    if '/' in line:\n",
        "      phs = line.split('/')\n",
        "    elif ';' in line:\n",
        "      phs = line.split(';')\n",
        "    else:\n",
        "      phs = [line]\n",
        "    # store phrases with metrics in the divided line\n",
        "    for i in phs:\n",
        "      if find_sub(i.lower(),ui_1):\n",
        "        if find_sub(i.lower(), av) and 'AV' not in p.keys():\n",
        "          p['AV']=i.strip().title()\n",
        "        if 'UI' not in p.keys():\n",
        "          p['UI']=i.strip().title()\n",
        "        else:\n",
        "          continue     \n",
        "      elif find_sub(i.lower(), ac) and 'AC' not in p.keys():\n",
        "        p['AC']=i.strip().title()\n",
        "      elif find_sub(i.lower(), av) and 'AV' not in p.keys():\n",
        "        p['AV']=i.strip().title()\n",
        "        if find_sub(i.lower(), ui_2, 1):\n",
        "          p['UI']=i.strip().title()\n",
        "      elif find_sub(i.lower(), ui_3) and 'UI' not in p.keys():\n",
        "        p['UI']=i.strip().title()\n",
        "  return p"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yLw9PFFL7E2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to extract property and store \n",
        "def catch_prop(i):\n",
        "  catch = 0\n",
        "  AV_flag = 0\n",
        "  UI_flag = 0\n",
        "  \n",
        "  prop = {}\n",
        "  lines = read_file(i)\n",
        "  # extract and store metric information\n",
        "  for j in lines:\n",
        "    j = re.sub('·','',j)\n",
        "    sents = tokenize.sent_tokenize(j)\n",
        "    # process metrics in one line\n",
        "    for k in sents:\n",
        "      m = re.search('attention:',k.lower())\n",
        "      if m is not None:\n",
        "        if not m.start():\n",
        "          prop.update(properties(k[m.end():], 0))\n",
        "          catch = 1\n",
        "    # process metrics in a specific section\n",
        "    if not catch:\n",
        "      if AV_flag:      \n",
        "        output = properties(tokenize.sent_tokenize(j),1)\n",
        "        real_output = {}\n",
        "        for key in output.keys():\n",
        "          if key not in prop.keys() or key != 'AV':\n",
        "            real_output[key] = output[key]\n",
        "        prop.update(real_output)\n",
        "        AV_flag = 0\n",
        "      if UI_flag:\n",
        "        output = properties(tokenize.sent_tokenize(j),1)\n",
        "        real_output = {}\n",
        "        for key in output.keys():\n",
        "          if key not in prop.keys() or key != 'AV':\n",
        "            real_output[key] = output[key]\n",
        "        prop.update(real_output)\n",
        "        UI_flag = 0\n",
        "      if re.search('[A-Za-z]',j) is not None:\n",
        "        s = re.search('[A-Za-z]',j).start()\n",
        "        j = j[s:]\n",
        "      if j.lower() == 'exploitability':\n",
        "        AV_flag = 1\n",
        "      if j.lower() == 'difficulty':\n",
        "        UI_flag = 1\n",
        "  \n",
        "  return prop"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53Jd-Wmixb62",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Researcher Information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfLUsknjQnmX",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2.1 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvGuJlvo9Svc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a list of keywords to extract researcher information\n",
        "# keywords for researchers\n",
        "r_kwrds = ['(self-)?report(ed|s)?', 'discover(ed|s)?', 'coordinate(d|s)?', \n",
        "           'identif(y|ied)?', '(self-)?disclose(d|s)?', 'sent', \n",
        "           'notif(y|ied)?', 'found','(is|are) credited']\n",
        "# keywords for researchers in passive tense           \n",
        "pass_r_kwds = []\n",
        "for i in range(len(r_kwrds)):\n",
        "  pass_r_kwds.append('(was|were|is|are) ' + r_kwrds[i] + ' by?')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwgAsXSZ9gVb",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2.2 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4OJvUAIZYSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to find the first verb in a sentence\n",
        "def find_first_verb(line):\n",
        "  text = nlp(line)\n",
        "  for i in text:\n",
        "    if i.pos_ == 'VERB':\n",
        "      return str(i)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwGe24jAqRMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to extract researchers in a line\n",
        "def div_researcher(line):\n",
        "\n",
        "  div = len(line)    \n",
        "  # process side case that begins with 'based'\n",
        "  if line.split()[0].lower() == 'based':\n",
        "    valid_sent = line.split(',')[-1]\n",
        "    verb = find_first_verb(valid_sent)\n",
        "    div = re.search(verb, valid_sent.lower()).start()\n",
        "    return valid_sent[:div].strip()\n",
        "  # process part of the sentence with researcher information in passive tense\n",
        "  for k in pass_r_kwds:\n",
        "    if re.search(k, line.lower()) is not None:\n",
        "      div = re.search(k, line.lower()).end()\n",
        "      res = line[div:]      \n",
        "      word_pos = [0,0]\n",
        "      text = nlp(res)\n",
        "      # begin collecting information from words of certain POSes\n",
        "      for t in text:\n",
        "        if word_pos[0] in ['CCONJ','ADP']:\n",
        "          conj = str(word_pos[1])\n",
        "          if t.pos_ in ['VERB','DET']:\n",
        "            div = re.search(conj, res.lower()).start()\n",
        "            res = res[:div].strip()\n",
        "            break   \n",
        "        word_pos = [t.pos_, t]\n",
        "      return res.strip()\n",
        "  # process another form of researcher information\n",
        "  for i in r_kwrds:\n",
        "    if re.search(i, line.lower()) is not None:\n",
        "      new_div = re.search(i, line.lower()).start()\n",
        "      if new_div < div:\n",
        "        div = new_div\n",
        "  line = line[:div].rstrip('to').strip().strip(',')\n",
        "  return line"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhVSNBEzBvgi",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Intrusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dylJGXl1_F8W",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3.1 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH-t8RZ6_LQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilities for extracting intrusions\n",
        "err = ['3.3 BACKGROUND','VULNERABILITY OVERVIEW','VULNERABILIY OVERVIEW']\n",
        "gap = ['VULNERABILITY DETAILS','BACKGROUND','RESEARCHER','MITIGATION',\n",
        "       'VUNLNERABILITY DETAILS','RESEARCHERS','EXPLOITABILITY']  \n",
        "sec = ['VULNERABILITY OVERVIEW','VUNLNERABILITY OVERVIEW',\n",
        "       'VULNERABILITY CHARACTERIZATION','VULNERABILITY',\n",
        "       'FILE AND DIRECTORY INFORMATION EXPOSURE CWE-538']\n",
        "regex_1 = '(result(s|ing)? in|causes?|allows?( attakers?)?|enables?| attackers? could|can) [A-Za-z ]+(,|.)'\n",
        "regex_2 = '( attackers?.+(could|can) )'"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvABIVhY_svy",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3.2 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQwyRs30sTb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A functin to pinpoint the sentence without CVE and not a divider as intrusion \n",
        "def if_only(lines):\n",
        "  rest_of = []\n",
        "  for i in lines:\n",
        "    if find_sub(i,['CVE','CWE','CVSS','---------']):\n",
        "      continue\n",
        "    elif check_line(i):\n",
        "      continue\n",
        "    else:\n",
        "      rest_of.append(i)\n",
        "  # return the first sentence pinpointed\n",
        "  if len(rest_of) == 1:\n",
        "    return rest_of[0]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4l975KXubye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to collect vulneralbilities from read lines\n",
        "def collect_vul(file_name):\n",
        "  attack_flag = 0\n",
        "  act = 0\n",
        "  sect_flag = 0\n",
        "  line_flag = 0\n",
        "  vul_detail = []\n",
        "\n",
        "  for j in read_file(file_name):\n",
        "    j = re.sub('·','',j)\n",
        "    # skip if the following line of a divider is wrongly placed\n",
        "    if act:\n",
        "      act = 0\n",
        "      if text_equal(j.upper(),err):\n",
        "        continue\n",
        "    # stop collecting when the end of the section occurs\n",
        "    if (text_equal(j.upper(),gap) or j == 'Exploitability') and attack_flag:\n",
        "      attack_flag = 0\n",
        "    # collect vulneralbility related phrases\n",
        "    if attack_flag: \n",
        "      s = find_key(j,['\\['])\n",
        "      line = j[:s]\n",
        "      vul_detail = vul_detail + tokenize.sent_tokenize(line)\n",
        "    # begin collecting after beginning dividers\n",
        "    if text_equal(j.upper(),sec):\n",
        "      attack_flag = 1\n",
        "      act = 1\n",
        "      sect_flag = 1\n",
        "    \n",
        "  return vul_detail"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5YgyxTaPuiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to get intrusion\n",
        "def get_intru(adv):\n",
        "  # initiate intrusion as a void string\n",
        "  intru=''\n",
        "  init_flag = 0\n",
        "  line_flag = 0\n",
        "  lines = []\n",
        "  # extract intrusion using regular expressions\n",
        "  index = 0\n",
        "  Vul = adv['Vulnerabilities']\n",
        "  \n",
        "  file_name = adv['Link'].split('/')[-1]+'.txt'\n",
        "  \n",
        "  for line in collect_vul(file_name):\n",
        "    lines.append(line)\n",
        "    # pinpoint intrusion using regular expression\n",
        "    if line_flag:\n",
        "      if re.search(regex_1, line.lower()) is not None:       \n",
        "        intru = line\n",
        "        line_flag = 0\n",
        "      elif re.search(regex_2, line.lower()) is not None:\n",
        "        intru = line\n",
        "        line_flag = 0\n",
        "    # select the only line not divider and without CVE info as intrusion\n",
        "    if check_line(line):\n",
        "      if intru == '':\n",
        "        only = if_only(lines)\n",
        "        if only is not None:\n",
        "          intru = only\n",
        "      line_flag = 1\n",
        "      if init_flag:\n",
        "        Vul[index]['Intrusion'] = intru\n",
        "        intru = ''\n",
        "        index += 1\n",
        "      init_flag = 1\n",
        "      lines = []\n",
        "  # select the only line not divider and without CVE info as intrusion\n",
        "  if init_flag:\n",
        "    if intru == '':\n",
        "      only = if_only(lines)\n",
        "      if only is not None:\n",
        "        intru = only\n",
        "    Vul[index]['Intrusion'] = intru   "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVkjJUbO_jGm",
        "colab_type": "text"
      },
      "source": [
        "# 4 Run the processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C2SHKVMCCEs",
        "colab_type": "text"
      },
      "source": [
        "## 4.0 Run Text Reading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0-VOAwNCUlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read all the files\n",
        "files_in_range=[]\n",
        "with open(\"valid_files.txt\", \"r\") as f:\n",
        "  files_in_range = f.readlines()\n",
        "for i in range(len(files_in_range)):\n",
        "  files_in_range[i] = files_in_range[i].rstrip()"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6mXp0H3CJfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read all the affected products sections\n",
        "advs=[]\n",
        "for fi in files_in_range:\n",
        "  advs.append(collect_lines(fi))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqYB56Ubrt00",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 Run Information Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGcx9pcGrx6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the process to get advisories\n",
        "advs = []\n",
        "for f in files_in_range:\n",
        "  adv = get_info(f)\n",
        "  advs.append(adv)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXTv6dP8tSsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Give each advisory a name\n",
        "for i in advs:\n",
        "  file_name = i['Title']+'.txt'\n",
        "  the_line = read_file(file_name)[1].strip()\n",
        "  i['Title'] = ('('+i['Title']+') '+the_line)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh_nozF9BvAN",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 Run Affected Products Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HePxI7v5BzLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run version conversion for all the advisories\n",
        "for i in advs:\n",
        "  file_name = i['Link'].split('/')[-1]+'.txt'\n",
        "  i['Affected Products'] = ver_con(file_name)\n",
        "  i['AP text'] = ', '.join(collect_lines(file_name)[1])"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQzTU2CgADbb",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 Run Metrics Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w_5M_4nAseA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run metric indentifying\n",
        "for i in advs:\n",
        "  i['AC'] = ''\n",
        "  i['AV'] = ''\n",
        "  i['UI'] = ''\n",
        "  file_name = i['Link'].split('/')[-1]+'.txt'\n",
        "  p = catch_prop(file_name)\n",
        "  i.update(catch_prop(file_name))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv32jG3iAeq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert metrics phrases into values\n",
        "for i in advs:\n",
        "  result = {'UI':{'text':i['UI'],'value':'None'}, \n",
        "            'AC':{'text':i['AC'],'value':''},\n",
        "            'AV':{'text':i['AV'],'value':''}}\n",
        "  expt = 0\n",
        "  for key in met_val.keys():\n",
        "    kwrds_met = met_val[key]\n",
        "    if i[key] == '':\n",
        "      continue\n",
        "    for kwrd in kwrds_met.keys():\n",
        "      if kwrd not in i[key].lower() and key == 'UI':\n",
        "        result['UI']['value'] = 'Required'\n",
        "      elif kwrd in i[key].lower():\n",
        "        result[key]['value'] = kwrds_met[kwrd]\n",
        "        if result[key]['value'] == 'case':\n",
        "          if result['UI']['value'] == 'None':\n",
        "            result[key]['value'] = 'High'\n",
        "          elif result['UI']['value'] == 'Required':\n",
        "            result[key]['value'] = 'Low'\n",
        "        break\n",
        "  i.update(result)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smiqVjvvAaQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Post process special cases of vulneralbility metrics \n",
        "for i in advs:\n",
        "  av = i['AV']['text'].lower()\n",
        "  for j in metric_sc:\n",
        "    if re.search(j,av) is not None:\n",
        "      if j == 'not ?.+(remotely| remotely)':\n",
        "        if 'physical access' in j:\n",
        "          i['AV']['value'] = 'Physical'\n",
        "        else:\n",
        "          i['AV']['value'] = 'Local'\n",
        "      elif j == 'some':\n",
        "        i['AV']['value'] = 'Network|Local'\n",
        "      elif j == 'except for':\n",
        "        i['AV']['value'] = 'Network|Local'\n",
        "      elif j == '(one|three) of ':\n",
        "        i['AV']['value'] = 'Network|Local'\n",
        "      else:\n",
        "        i['AV']['value'] = 'Network'\n",
        "      break\n",
        "  ac = i['AC']['text'].lower()\n",
        "  if re.search('would not be difficult', ac) is not None:\n",
        "    i['AC']['value'] = 'Low'\n",
        "  ui = i['UI']['text'].lower()\n",
        "  if re.search(' an anthenticated user ',ui) is not None:\n",
        "    i['UI']['text'] = ''\n",
        "    i['UI']['value'] = 'None'"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1nLmt9NACUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Post process special cases of vulneralbility metrics \n",
        "for adv in advs:\n",
        "  i = adv['Link'].split('/')[-1]+'.txt'\n",
        "  rf = read_file(i)\n",
        "  num = len(rf)\n",
        "  if not num:\n",
        "    continue\n",
        "  j = 0\n",
        "  v = []\n",
        "  while j < num:\n",
        "    l = rf[j]\n",
        "    if l.lower() == 'exploitability':\n",
        "      temp = j+1\n",
        "      nxt_one = rf[temp]\n",
        "      if '---------' in nxt_one:\n",
        "        temp = temp+1\n",
        "        nxt_one = rf[temp]\n",
        "      nxt = nxt_one\n",
        "      while re.search('exisi?t(e|a)nce of (report|exp(loit|olit))',nxt.lower()) is None:\n",
        "        if '---------' not in nxt:\n",
        "          v.append(nxt)\n",
        "        temp += 1\n",
        "        nxt = rf[temp]\n",
        "      break\n",
        "    else:\n",
        "      j += 1\n",
        "  if len(v):\n",
        "    flag = 0\n",
        "    for t in typical:\n",
        "      if re.search(t,v[0]) is None:\n",
        "        flag += 1\n",
        "      if flag == len(typical):\n",
        "        text = ' '.join(v)\n",
        "        adv['AV']['text'] = text\n",
        "        adv['AV']['value'] = 'Network|Local'"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtVn6G2kAKoj",
        "colab_type": "text"
      },
      "source": [
        "## 4.4 Run Researcher and Intrusion Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4ez0SKY_iKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# correct extracted researcher information\n",
        "for i in advs:\n",
        "  if i['Researcher'].upper() == i['Researcher']:\n",
        "    i['Researcher'] = ''\n",
        "  elif '-----' in i['Researcher']:\n",
        "    catch = 0\n",
        "    file_name = i['Link'].split('/')[-1]+'.txt'\n",
        "    for line in read_file(file_name):\n",
        "      if catch:\n",
        "        catch -= 1\n",
        "        if not catch:\n",
        "          i['Researcher'] = line\n",
        "      if re.sub('[^A-Za-z]','',line.lower()) == 'researcher':\n",
        "        catch = 2\n",
        "# Run researcher extraction\n",
        "for i in advs:\n",
        "  R = i['Researcher']\n",
        "  if R != '':\n",
        "    i['Researcher text'] = R\n",
        "    i['Researcher'] = div_researcher(R)\n",
        "# Run intrusion extracting for advisories\n",
        "for i in advs:\n",
        "  get_intru(i)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WlcRpBjSelf",
        "colab_type": "text"
      },
      "source": [
        "# 5 Output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVk2S06411aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = {}\n",
        "doc['Advisories'] = advs"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5lfLRYoSaL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('readable_file.json', 'w') as fp:\n",
        "  json.dump(doc,fp)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHWSJKjTSkrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('output_advisories.json', 'w') as fp:\n",
        "  json.dump(doc, fp, indent = 4)"
      ],
      "execution_count": 125,
      "outputs": []
    }
  ]
}