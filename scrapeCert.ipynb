{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrapeCert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oXdFTIofjM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from zipfile import ZipFile\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDb60rhI6IYQ",
        "colab_type": "text"
      },
      "source": [
        "# 0 Collect links and file names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdjMAP9jgyhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collect all links to pages \n",
        "urls=[]\n",
        "# manual put the number of pages on CERT\n",
        "for i in range(57):\n",
        "  urls.append(\"https://www.us-cert.gov/ics/advisories?page={}\".format(i))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbuz_CBchQ4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d19ba9b5-8570-41ec-eb35-915f641a2435"
      },
      "source": [
        "# Collect all links to reports\n",
        "docs=[]\n",
        "for i in urls:\n",
        "  page = requests.get(i)\n",
        "  soup = BeautifulSoup(page.text, \"html.parser\", from_encoding='utf-8')\n",
        "  # Get links to reports on each page\n",
        "  for div in soup.find_all(name='span', attrs={'class':'field-content'}, href=False):\n",
        "    for subdiv in div.find_all(name='a',href=True):\n",
        "      name = subdiv['href']    \n",
        "      link_2_doc = 'https://www.us-cert.gov{}'.format(name)\n",
        "      docs.append(link_2_doc)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:179: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
            "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTWTnkcLBFQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save file names for later use\n",
        "file_names=[]\n",
        "for i in docs:\n",
        "  file_names.append(i.split('/')[-1]+'.txt')\n",
        "# write file names into a .txt file\n",
        "with open(\"FILES.txt\", \"w\") as f:\n",
        "  for i in file_names:\n",
        "    f.write(i+'\\n')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhEfuSNb7Ws4",
        "colab_type": "text"
      },
      "source": [
        "# 1 Collect Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IAE1dBy75Ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilities for text collection\n",
        "nest_layer_1 = ['li','h2','h3','h4','p','ol','div','ul']\n",
        "nest_layer_2 = ['h2','h3','h4','p','ol','div','ul']\n",
        "name = ['h2','h3','h4','p','ol','div','ul']\n",
        "collect_tag = ['h2','h3','h4','p','ol','ul','div']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD9Oumqa4OZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to remove footnotes by converting it to footnote tags\n",
        "def remove_foot(cont,append_flag,soup):\n",
        "\n",
        "  footnote_flag = 0\n",
        "\n",
        "  # use footnote to indicate the indexes of footnotes\n",
        "  footnote=\" [Footnote:\"\n",
        "  # collect and clean footnotes\n",
        "  for ft in cont.find_all(name='a', attrs = {'class':\"see-footnote\"}):\n",
        "    # record the indexes of footnotes\n",
        "    footnote_flag = 1\n",
        "    footnote = footnote + ft.get_text() + ','\n",
        "    # replace the footnode with tags with no content\n",
        "    a_tag = ft\n",
        "    new_tag = soup.new_tag(\"b\")\n",
        "    new_tag.string = \"\"\n",
        "    a_tag.replace_with(new_tag)\n",
        "  # remove surplus comma and close the bracket\n",
        "  footnote = footnote[:-1]+']'\n",
        "  \n",
        "  # clean symbols between footnotes\n",
        "  for in_sup in cont.find_all(name='sup'):\n",
        "    # replace the symbol with no-content tags\n",
        "    sup_tag = in_sup\n",
        "    n_tag = soup.new_tag(\"c\")\n",
        "    n_tag.string = \"\"\n",
        "    sup_tag.replace_with(n_tag)\n",
        "\n",
        "  # replace 'br' section with new line indicator\n",
        "  for line_br in cont.find_all(name='br'):\n",
        "    sup_tag = line_br\n",
        "    n_tag = soup.new_tag(\"bk\")\n",
        "    n_tag.string = \"\\n\"\n",
        "    sup_tag.replace_with(n_tag)\n",
        "\n",
        "  return cont.get_text()+footnote_flag*footnote"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqoCj6AwAOZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to remove tags from a text body\n",
        "def remove_tag(elem,tags,soup):\n",
        "\n",
        "  for tag in tags:\n",
        "    for incl_tag in elem.find_all(name=tag):\n",
        "      spl_tag = incl_tag\n",
        "      n_tag = soup.new_tag(\"clr\")\n",
        "      n_tag.string = \"\"\n",
        "      spl_tag.replace_with(n_tag)\n",
        "      \n",
        "  return elem"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPlZ04CEJz5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to remove nested sections to avoid duplicates\n",
        "def remove_nest(cont,append_flag,soup):\n",
        "\n",
        "  content = ['']\n",
        "  \n",
        "  ul_flag = 0\n",
        "  \n",
        "  for elem in cont.find_all(name=nest_layer_1, href=False):\n",
        "\n",
        "    elem_cont = elem\n",
        "    if elem.name != 'li':\n",
        "      ul_flag += 1\n",
        "\n",
        "    if len(elem.find_all(name=nest_layer_2, href=False)): \n",
        "      elem_cont = remove_tag(elem_cont,name,soup)\n",
        "\n",
        "    if elem.name == 'ul':\n",
        "      continue\n",
        "    \n",
        "    line = remove_foot(elem, append_flag, soup)\n",
        "\n",
        "    if elem.name == 'li':\n",
        "      line = 'Â·' + line\n",
        "    content.append(line)\n",
        "  \n",
        "  name.append('li')\n",
        "  cont = remove_tag(cont,name,soup)\n",
        "  content[0] = remove_foot(cont, append_flag, soup)\n",
        "  if re.sub(r'\\n','',content[0]) == '':\n",
        "    content[0] = re.sub(r'\\n','',content[0])\n",
        "  if content[0] != '':\n",
        "    content = ['']+content\n",
        "  content.append(ul_flag)\n",
        "\n",
        "  return content"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W_pMj7LyP7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define collect_doc that derive reports from each link\n",
        "def collect_doc(link):\n",
        "  \n",
        "  # use div_count to only collect text from report part \n",
        "  div_count = 1\n",
        "  # initialize content to contain all the texts in this report\n",
        "  content = []\n",
        "\n",
        "  page = requests.get(link)\n",
        "  soup = BeautifulSoup(page.text, \"html.parser\", from_encoding='utf-8')\n",
        "  file_name = link.split(\"/\")[-1]\n",
        "\n",
        "  for div_t in soup.find_all(name='div', attrs={'id':'submitted meta-text'}):\n",
        "    info_date = div_t.get_text().split()[5]\n",
        "    if info_date > 2011:\n",
        "      files_in_range.append(\"{}.txt\".format(file_name))\n",
        "\n",
        "  with open(\"{}.txt\".format(file_name), \"w\") as f:\n",
        "    # collect the title of the report\n",
        "    for div_f in soup.find_all(name='div', attrs={'id':'ncas-header'}):\n",
        "      # find the section that contains title information\n",
        "      for title in div_f.find_all(name=['h1','h2'], href=False):\n",
        "        titles = [title.get_text()+'\\n'*(len(content)==0)]\n",
        "        content = content + titles\n",
        "    # collect the whole report section\n",
        "    for div in soup.find_all(name='div', attrs={'class':'field field--name-body field--type-text-with-summary field--label-hidden field--item'}):\n",
        "      # use ul_flag to avoid duplicate texts\n",
        "      ul_flag = 0\n",
        "      # use append_flag to only collect text from report part \n",
        "      append_flag = 1\n",
        "      if div_count != 5:\n",
        "        append_flag = 0\n",
        "      div_count += 1\n",
        "      # collect lines with specific tags that corresponds with content\n",
        "      for cont in div.find_all(name=collect_tag, href=False):\n",
        "        # use bulletings to contain bulleting items\n",
        "        bulletings=[]\n",
        "        # decrease ul_flag and skip the line to avoid duplicates\n",
        "        if ul_flag:\n",
        "          ul_flag -= 1\n",
        "          continue\n",
        "\n",
        "        # collect paragraph that is not bulleting\n",
        "        if append_flag:\n",
        "          paragraph = remove_nest(cont, append_flag, soup)\n",
        "          if paragraph[-1]:\n",
        "            ul_flag += paragraph[-1]\n",
        "          # add paragraph to content \n",
        "          content = content + paragraph[:-1]\n",
        "    \n",
        "    # write the txt file\n",
        "    for i in content:\n",
        "      if type(i) == int:\n",
        "        continue\n",
        "      f.write(i+\"\\n\"*(content.index(i)<len(content)-1))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiGR22yg7rZG",
        "colab_type": "text"
      },
      "source": [
        "# 2 Run the process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUAIWXq1KrEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collect reports from links\n",
        "for i in docs:\n",
        "  print(i)\n",
        "  collect_doc(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeKLoeJdKCBL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a23c2184-c29c-4fb9-a5e7-5dfdabf79706"
      },
      "source": [
        "# Record the time of reports\n",
        "files_in_range = {}\n",
        "for link in docs:\n",
        "  page = requests.get(link)\n",
        "  time.sleep(0.3+random.randint(1,5)/10)\n",
        "  soup = BeautifulSoup(page.text, \"html.parser\", from_encoding='utf-8')\n",
        "  file_name = link.split(\"/\")[-1]\n",
        "  for div_t in soup.find_all(name='div', attrs={'class':'submitted meta-text'}):\n",
        "    info_date = div_t.get_text().split()[5]   \n",
        "    files_in_range[\"{}.txt\".format(file_name)] = info_date"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:179: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
            "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oSr8EqZ0GBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove reports generated before 2011\n",
        "with open(\"valid_files.txt\", \"w\") as f:\n",
        "  for i in files_in_range.items():\n",
        "    if int(i[1])>2011:\n",
        "      f.write(i[0]+'\\n')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRkHBOxqxKsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Zip all the reports\n",
        "with ZipFile(\"reports.zip\",\"w\") as newZip:\n",
        "  for i in docs:\n",
        "    name = i.split(\"/\")[-1]\n",
        "    file_name = \"{}.txt\".format(name)\n",
        "    newZip.write(file_name)"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}